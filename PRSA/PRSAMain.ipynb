{"cells":[{"cell_type":"markdown","metadata":{"id":"fF7Q2RzLGBmX"},"source":["**IMPORTANT NOTES**\n","\n","- **Comments with two hash symbols (##) are notes for whoever runs this notebook. They are cell-by-cell instruction of what to modify and what to keep as it is.**\n","\n","- **To avoid out-of-memory issues, it is strongly recommended not to run unneeded cells of code. Some of them are reported for demonstration purposes only and does not need to be run. Please follow the running instructions carefully. Running additional or unnecessary code can lead to excessive memory usage, causing Colab to disconnect.**\n"]},{"cell_type":"markdown","metadata":{"id":"Jznx-UrDgGn6"},"source":["---\n","# **DRIVE MOUNTING AND LOGGING**"]},{"cell_type":"markdown","metadata":{"id":"80-zCWd6pyXy"},"source":["This section is responsible for installing the requirements and mounting the Google Drive if you run the notebook in Colab. It ensures that the required dependencies are available and the notebook can access the dataset from Google Drive."]},{"cell_type":"markdown","metadata":{"id":"lNENLLhXtA7d"},"source":["> ## **Drive Mounting and CWD**\n",">\n","> **_Important:_**  \n",">\n","> **If the \"Tabular_Transformer\" folder is a shared folder, you will need to create a shortcut to it in your own Drive. You can do this by navigating to the shared folder, right-clicking, and selecting \"Add shortcut to Drive\". Once you add the shortcut to your Drive, you should be able to access it from Colab as described below. Be sure to set the correct ROOT_DIR path.**"]},{"cell_type":"code","execution_count":1,"metadata":{"id":"bffUFyBjQo-P","executionInfo":{"status":"ok","timestamp":1705232282138,"user_tz":-60,"elapsed":42044,"user":{"displayName":"paolo","userId":"03714737150223700499"}}},"outputs":[],"source":["%%capture\n","## CREATE A SHORTCUT TO THE DRIVE AND RUN THIS CELL\n","import os\n","from google.colab import drive\n","drive.mount('/content/drive', force_remount=True)\n","\n","## ADJUST THE PATHS IF NEEDED\n","ROOT_DIR='/content/drive/MyDrive/Tabular_Transformer/PRSA'\n","RAW_DATA_DIR = os.path.join(ROOT_DIR, 'data/raw')\n","PROCESSED_DATA_DIR = os.path.join(ROOT_DIR, 'data/processed')\n","VOCAB_DIR = os.path.join(ROOT_DIR, 'vocab')\n","\n","# navigate to the root directory and run the setup.py file to install the required dependencies\n","os.chdir(ROOT_DIR)\n","!pip install -r requirements.txt"]},{"cell_type":"markdown","metadata":{"id":"QGtdK_YmgqMO"},"source":["> ## **Logging**\n",">\n","> This cell initializes a basic logging configuration to monitor the activities."]},{"cell_type":"code","execution_count":2,"metadata":{"id":"b4Y5tvkyF6O8","executionInfo":{"status":"ok","timestamp":1705232288897,"user_tz":-60,"elapsed":6762,"user":{"displayName":"paolo","userId":"03714737150223700499"}}},"outputs":[],"source":["## RUN THIS CELL\n","from src.utils import setup_logging\n","\n","setup_logging()"]},{"cell_type":"markdown","metadata":{"id":"x4oFf2QXNceY"},"source":["---\n","# **DATA EXTRACTION**\n","\n"]},{"cell_type":"markdown","metadata":{"id":"H9iEENAfGnX-"},"source":["> ## **Data Extractor Class**\n",">\n","> The DataExtractor class is designed to extract and load the data. Here's a summary of what the class does:\n",">\n","> - **Class Initialization:**\n",">   - The constructor initializes the object with the directory path where the data is stored and the number of samples per file to extract (if the directory contains multiple files).\n",">   - The inputs are validated and the data is extracted.\n",">\n","> - **Data Extraction:**\n",">   - The *_extract_data* private method allows to extract the data from all the files in the data directory, loading everything into a single unified pandas DataFrame. The Pollution Dataset used for this project is a public UCI dataset for predicting both PM2.5\n","and PM10 air concentration for 12 monitoring sites (12 files), each containing around 35k entries (rows). [Padhi et al.,  2021]\n",">\n",">   - The *_load_from_csv* private method loads the csv data at the specified location into a pandas DataFrame. You can either specify the number of samples per file to extract or set it to None to extract all the samples.\n",">\n",">   - The *_merge_time_col* private method merge various time-related columns into a single 'TIMESTAMP' column. This process ensures a unified and consistent time representation, which is crucial for time-series analysis.\n",">\n",">   - The *_split_data* private method splits data based on Station and TIMESTAMP columns. The data is splitted into training, validation, and test sets to avoid data leakage between the sets during preprocessing.\n",">\n","> **_Important_:**\n",">\n","> **In the next section, the preprocessed data will be loaded to simplify the run of the notebook, thus there's no need to run the code cell below.**\n",">\n","> **However, feel free to review the code below to understand how to use the DataExtractor class, but running it is not required and can lead to memory issues.**\n","\n","\n","[Padhi et al.,  2021]: https://arxiv.org/abs/2011.01843 \"Padhi, I., Schiff, Y., Melnyk, I., et al. (2021). Tabular Transformers for Modeling Multivariate Time Series. arXiv:2011.01843 [cs.LG]\""]},{"cell_type":"code","source":["## NO NEED TO RUN THIS CELL\n","from src.data.extraction import DataExtractor\n","\n","# if None, extract all the samples\n","samples_per_file = None\n","\n","extractor = DataExtractor(data_root_dir=RAW_DATA_DIR,\n","                          samples_per_file=samples_per_file,\n","                          train_size=0.8,\n","                          val_size=0.1)\n","train_data = extractor.train_data\n","val_data = extractor.val_data\n","test_data = extractor.test_data"],"metadata":{"id":"4iN4OKgPFpa8","executionInfo":{"status":"ok","timestamp":1705232308578,"user_tz":-60,"elapsed":19688,"user":{"displayName":"paolo","userId":"03714737150223700499"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"87250151-da53-4cce-c390-d406f0b30a8f"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stderr","text":["2024-01-14 11:38:10,298 - INFO - numexpr.utils - NumExpr defaulting to 2 threads.\n","2024-01-14 11:38:11,565 - INFO - src.data.extraction - Initializing the DataExtractor...\n","Data Extraction:: 100%|██████████| 12/12 [00:15<00:00,  1.29s/it]\n","2024-01-14 11:38:27,555 - INFO - src.data.extraction - Successfully extracted 12 DataFrame. Train DataFrame has 336612 rows. Validation DataFrame has 42072 rows. Test DataFrame has 42084 rows.\n","\n","2024-01-14 11:38:27,559 - INFO - src.data.extraction - DataExtractor successfully initialized.\n","\n"]}]},{"cell_type":"markdown","metadata":{"id":"Zx_RWpB-Ac0r"},"source":["---\n","# **THE DATASET**"]},{"cell_type":"markdown","source":["> ## **PRSADataset Class**\n",">\n","> The PRSADataset class is designed to take raw data and prepare it for train and test phases.\n",">\n","> - **Class Initialization**:\n","    - Initializes with a DataFrame and the dataset mode (train, val, test).\n","    When the mode is 'train', the vocabulary is saved in the specified directory and transformations are applied based on statistical information from the training data.\n","    When the mode is 'val' or 'test', the vocabulary is loaded from the specified directory and transformations are applied based on the statistical information from the training data.\n","    - Other arguments include: directory paths where to save the vocabulary and the class instance, the columns to discretize, the columns to drop, the target columns, the sequence length and the stride.\n","    - The attributes are validated to ensure that the class is initialized correctly.\n","    - The data is preprocessed based on the mode.\n","    - The data is tokenized using the vocabulary.\n","    - The samples and targets are prepared for time-series analysis based on the sequence length and the stride.\n",">\n","> - **Data Preprocessing**:\n","    - Processes the input data based on the specified mode (train, val, test).\n","    - In training mode, the data is discretized and a vocabulary is created and saved.\n","    - In validation and test modes the data is discretized based on the bin edges found with the training data, then the vocabulary created with the training data is loaded and applied to the val/test data.\n",">\n","> - **Discretization**:\n","    - Compute the number of bins to discretize the dataset based on its interquartile range (IQR).\n","    - The method uses the Freedman-Diaconis Rule to compute the width of each bin. The rule is robust to outliers and is given as  $$ \\text{Bin width} = \\frac{2 \\times \\text{IQR}}{\\sqrt[3]{\\text{num observations}}} $$    \n","    - The number of bins is calculated as\n","$$ \\text{Number of bins} = \\frac{\\text{max value} - \\text{min value}}{\\text{Bin width}} $$\n","    - The bin labels are computed based on the number of bins specified.\n","    - The data is discretized by assigning each value to the closest bin label.\n","    - The bin labels are saved in a json file and loaded when the mode is 'val' or 'test'.\n",">\n","> - **Vocabulary**:\n","    - Uses the *Vocab* class to manage the vocabulary of the dataset.\n","    - The vocabulary is created based on the columns specified in *cols_for_vocab*.\n","    - Each column has its own vocabulary, with the following structure: {token: [global_index, local_index]}\n","    - The vocabulary is created and saved when the mode is 'train', otherwise it is loaded from the specified directory.\n",">\n","> - **Tokenization**:\n",">   - Maps the tokens in the columns specified in *cols_for_vocab* to the corresponding global indices.\n","> - **Sample Preparation**:\n","    - Structures the data into samples and targets with a format suitable for time-series analysis.\n","    - A single sample contains seq_len*(ncols+1) token ids. The shape of the sample is (seq_len, ncols+1).\n","    - The number of samples obtained in the end depends on the stride and on the number of subsequent rows considered for each sample (sequence length).\n",">\n","> - **Saving and Loading the Dataset**:\n","    - It's possible to save the entire class instance using pickle for efficient storage and retrieval.\n","    - It's possible to load the class instance, a static method is provided for this purpose.\n",">\n","> **_Important_:**\n",">\n","> **Run the following cell keeping the 'load' argument to 'True' to load the preprocessed data we used for our experiments.\n","If you want to preprocess the data again, change the 'load' argument to 'False'. Keep in mind that this will take more time.**\n"],"metadata":{"id":"npKRiV5pPVcv"}},{"cell_type":"code","source":["## RUN THIS CELL - NOTHING TO CHANGE\n","from src.data.dataset import PRSADataset\n","\n","load = True\n","if not load:\n","    train_dataset = PRSADataset(data=train_data,\n","                                mode='train',\n","                                vocab_dir=VOCAB_DIR,\n","                                save_dir=PROCESSED_DATA_DIR)\n","    val_dataset = PRSADataset(data=val_data,\n","                              mode='val',\n","                              vocab_dir=VOCAB_DIR,\n","                              save_dir=PROCESSED_DATA_DIR)\n","    test_dataset = PRSADataset(data=test_data,\n","                               mode='test',\n","                               vocab_dir=VOCAB_DIR,\n","                               save_dir=PROCESSED_DATA_DIR)\n","else:\n","    train_dataset = PRSADataset.load(data_dir=PROCESSED_DATA_DIR,\n","                                     mode='train')\n","    val_dataset = PRSADataset.load(data_dir=PROCESSED_DATA_DIR,\n","                                   mode='val')\n","    test_dataset = PRSADataset.load(data_dir=PROCESSED_DATA_DIR,\n","                                    mode='test')"],"metadata":{"id":"a3rBPe1Z3xwt","executionInfo":{"status":"ok","timestamp":1705232322908,"user_tz":-60,"elapsed":14333,"user":{"displayName":"paolo","userId":"03714737150223700499"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"7ab2fca0-b8cd-4dfa-aeac-899385c8e0e3"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stderr","text":["2024-01-14 11:38:29,298 - INFO - src.data.dataset - Loading PRSA Dataset...\n","2024-01-14 11:38:37,026 - INFO - src.data.dataset - Class instance successfully loaded.\n","\n","2024-01-14 11:38:37,035 - INFO - src.data.dataset - Loading PRSA Dataset...\n","2024-01-14 11:38:39,433 - INFO - src.data.dataset - Class instance successfully loaded.\n","\n","2024-01-14 11:38:39,436 - INFO - src.data.dataset - Loading PRSA Dataset...\n","2024-01-14 11:38:41,784 - INFO - src.data.dataset - Class instance successfully loaded.\n","\n"]}]},{"cell_type":"code","source":["## RUN THIS CELL - NOTHING TO CHANGE\n","# -----------------------------------------------------------------------------------------------------------\n","# NOTE: This cell shows the structure of a single sample from the PRSADataset. Each sample comprises multiple rows,\n","# each with 12 columns. The data in these columns has been discretized, mapped to the nearest bin edge, and\n","# then converted to indices. Observing this sample helps in understanding the preprocessing steps applied to the\n","# dataset, such as discretization and tokenization, and how the data is presented to the data collator.\n","# -----------------------------------------------------------------------------------------------------------\n","first_sample = train_dataset[0][0]\n","first_sample_labels = train_dataset[0][1]\n","print(f\"First sample:\\n {first_sample}\")\n","print(f\"Shape [seq_len, num_cols]: {first_sample.shape}\\n\")\n","print(f\"Targets associated to the first sample:\\n {first_sample_labels}\\n\")\n","print(f\"Shape:\\n {first_sample_labels.shape}\")"],"metadata":{"id":"TSHYex_fG9hm","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1705232322908,"user_tz":-60,"elapsed":6,"user":{"displayName":"paolo","userId":"03714737150223700499"}},"outputId":"952b5e74-b0d6-4d62-c309-25ad09b6d312"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["First sample:\n"," tensor([[  7,  86, 144, 183, 304, 318, 334, 348, 373, 381, 412,   1],\n","        [  7,  86, 144, 183, 304, 318, 334, 348, 373, 381, 413,   1],\n","        [  8,  87, 144, 184, 304, 318, 334, 348, 373, 381, 412,   1],\n","        [  9,  88, 144, 185, 304, 318, 334, 349, 373, 381, 414,   1],\n","        [ 10,  88, 144, 185, 305, 319, 334, 350, 373, 381, 413,   1],\n","        [ 11,  89, 145, 186, 305, 319, 334, 351, 373, 381, 413,   1],\n","        [ 11,  90, 146, 187, 305, 319, 334, 352, 373, 381, 415,   1],\n","        [ 12,  91, 146, 188, 305, 319, 334, 351, 373, 381, 412,   1],\n","        [ 13,  92, 146, 189, 304, 319, 334, 351, 373, 381, 412,   1],\n","        [ 10,  93, 145, 190, 304, 319, 334, 353, 373, 381, 413,   1]])\n","Shape [seq_len, num_cols]: torch.Size([10, 12])\n","\n","Targets associated to the first sample:\n"," tensor([[4., 4.],\n","        [8., 8.],\n","        [7., 7.],\n","        [6., 6.],\n","        [3., 3.],\n","        [5., 5.],\n","        [3., 3.],\n","        [3., 6.],\n","        [3., 6.],\n","        [3., 8.]])\n","\n","Shape:\n"," torch.Size([10, 2])\n"]}]},{"cell_type":"markdown","metadata":{"id":"wc_PufOvIo2G"},"source":["---\n","# **VOCABULARY**"]},{"cell_type":"markdown","source":["> ## **Vocabulary Class**\n",">\n","> The *Vocab* class is designed to manage, create, save and load vocabularies.\n",">\n","> - **Class Initialization**:\n","    - The class starts by defining a set of custom special tokens as class-level constants.\n","    - The vocab object accepts columns from the DataFrame to create the vocabulary, the actual data, a directory to save the vocabulary, and target columns.\n","    - Various attributes are first initialized and validated to ensure they are correctly provided.\n","    - Special tokens are initialized and added to the vocabulary.\n","    - Vocabularies are created based on the provided data.\n",">\n","> - **Vocabulary Creation**:\n","    - The vocabulary is constructed using the unique values from the provided columns in the data. Each unique value is added to the vocabulary with a corresponding tag (the field name), global id (the index considering all the tokens of the vocabulary) and local id (the index considering only the tokens in the current field).\n","    - Vocabulary structure: {column tag: {token: [global_id, local_id]}}\n","    - id2token structure: {global_id: [token, tag, local_id]}\n",">    \n","> - **Utility Methods**:\n","    - The class also provides a method to retrieve the global id of a token, a method to retrieve the token corresponding to a global id, a method to map between global and local ids using a lookup tensor and two methods to save/load the vocabulary.\n",">   \n","> - **Vocabulary Summary Display**:\n","    - The Vocab class includes the print_vocab_summary method. This method allows for a detailed display of various statistics and characteristics of the vocabulary. It's possible to print special tokens, sample tokens from each column, the size of the vocabulary for each column, the data types of each column, and the total length of the vocabulary. You can specify the sample size and token limit per column.\n",">\n","> **_Important_**:\n",">\n","> **Run the following cell to extract the vocabulary from the training set.**\n"],"metadata":{"id":"kowP5A45nRcb"}},{"cell_type":"code","source":["## RUN THIS CELL\n","vocab = train_dataset.vocab\n","## ADJUST THE PARAMETERS AS YOU WANT (keep as they are to print everything)\n","vocab.print_vocab_summary(print_special_tokens=True,\n","                          print_sample_tokens=True,\n","                          sample_size=5,\n","                          token_limit_per_column=5,\n","                          print_vocab_size_per_column=True,\n","                          print_column_data_types=True,\n","                          print_vocab_length=True)"],"metadata":{"id":"ammRhMlIBRWn","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1705232322908,"user_tz":-60,"elapsed":3,"user":{"displayName":"paolo","userId":"03714737150223700499"}},"outputId":"8715dfb0-63a6-4b13-8c9c-d34e96f0dad6"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["Special tokens: ['[UNK]', '[SEP]', '[PAD]', '[CLS]', '[MASK]', '[START]', '[END]']\n","\n","Sampling from the Vocabulary:\n","\n","COLUMN_TAG: SO2\n","TOKEN: [GLOBAL IDX, LOCAL IDX]\n","4.0: [7, 0]\n","5.0: [8, 1]\n","11.0: [9, 2]\n","12.0: [10, 3]\n","18.0: [11, 4]\n","\n","\n","COLUMN_TAG: wd\n","TOKEN: [GLOBAL IDX, LOCAL IDX]\n","NNW: [412, 0]\n","N: [413, 1]\n","NW: [414, 2]\n","NNE: [415, 3]\n","ENE: [416, 4]\n","\n","\n","COLUMN_TAG: PRES\n","TOKEN: [GLOBAL IDX, LOCAL IDX]\n","1023.1: [318, 0]\n","1026.4: [319, 1]\n","1020.5: [320, 2]\n","1018.0: [321, 3]\n","1015.6: [322, 4]\n","\n","\n","COLUMN_TAG: TIMESTAMP\n","TOKEN: [GLOBAL IDX, LOCAL IDX]\n","0.0: [381, 0]\n","0.026666286398768335: [382, 1]\n","0.053332572797534894: [383, 2]\n","0.07999885919630323: [384, 3]\n","0.10666514559507156: [385, 4]\n","\n","\n","COLUMN_TAG: DEWP\n","TOKEN: [GLOBAL IDX, LOCAL IDX]\n","-18.2: [334, 0]\n","-14.1: [335, 1]\n","-10.2: [336, 2]\n","-6.5: [337, 3]\n","-3.2: [338, 4]\n","\n","\n","Number of tokens in column 'SO2': 79\n","Number of tokens in column 'NO2': 58\n","Number of tokens in column 'CO': 39\n","Number of tokens in column 'O3': 121\n","Number of tokens in column 'TEMP': 14\n","Number of tokens in column 'PRES': 16\n","Number of tokens in column 'DEWP': 14\n","Number of tokens in column 'WSPM': 25\n","Number of tokens in column 'RAIN': 8\n","Number of tokens in column 'TIMESTAMP': 31\n","Number of tokens in column 'wd': 17\n","\n","\n","Data Types per Column:\n","Column 'SO2': float64\n","Column 'NO2': float64\n","Column 'CO': float64\n","Column 'O3': float64\n","Column 'TEMP': float64\n","Column 'PRES': float64\n","Column 'DEWP': float64\n","Column 'WSPM': float64\n","Column 'RAIN': int64\n","Column 'TIMESTAMP': float64\n","Column 'wd': object\n","\n","\n","Total Length of the Vocabulary: 429\n"]}]},{"cell_type":"markdown","metadata":{"id":"ck4aM7YIGQsf"},"source":["---\n","# **TOKENIZER AND DATA COLLATOR**"]},{"cell_type":"markdown","source":["> ## **Tokenizer**\n",">\n","> In this cell we initialize the BERT tokenizer. The tokenizer is initialized with the vocabulary file from the vocab object.\n","This tokenizer is used in the CustomDataCollator class to pad the input ids and mask tokens for MLM tasks.\n",">\n","> **_Important_**:\n",">\n","> **Run the following cell to initialize the tokenizer.**"],"metadata":{"id":"FHPOff5z7ZFX"}},{"cell_type":"code","execution_count":7,"metadata":{"id":"lW5O4HPSIu17","executionInfo":{"status":"ok","timestamp":1705232325434,"user_tz":-60,"elapsed":2528,"user":{"displayName":"paolo","userId":"03714737150223700499"}}},"outputs":[],"source":["## RUN THIS CELL - NO CHANGES NEEDED\n","from transformers import BertTokenizerFast\n","\n","tokenizer = BertTokenizerFast(vocab_file=vocab.vocab_file_for_bert,\n","                              do_lowercase=False,\n","                              **vocab.get_special_tokens())"]},{"cell_type":"markdown","source":["> ## **Custom Data Collator**\n",">\n","> The *CustomDataCollator* class is designed to handle tabular and time series data, preparing the samples, the targets and the masked language model labels.\n",">\n","> - **Hugging Face Compatibility**:\n","    - It inherits from *DataCollatorForLanguageModeling*, ensuring compatibility with Hugging Face.\n","    - The class requires a Bert tokenizer for padding and masking the input ids.\n",">    \n","> - ***`__call__`* Method**:\n","    - Each row represent a collection of features (already converted to indices). Remember that the sequence lenght parameter of the dataset defines the number of subsequent rows that constitute a single sample.\n","    - The collator efficiently groups multiple samples into a batch, the batch received by the model has shape [batch, seq_len, ncols+1].\n","    - The method can handle both MLM and regression tasks.\n","    - In MLM mode, it masks certain tokens in the input ids based on mlm_probability and returns the labels.\n","    - In regression mode, it returns the targets.\n","> - For source code and usage, refer to the [Hugging Face's documentation](https://github.com/huggingface/transformers/blob/v4.34.0/src/transformers/data/data_collator.py#L607).\n",">\n","> **_Important_:**\n",">\n","> **In the following cell, we demonstrate how to use the CustomDataCollator class for MLM and regression tasks.\n","> Note that there's no need to create the collator object manually as the CustomDataCollator object will be created in the training manager.**\n"],"metadata":{"id":"W6J9_N8qBc7A"}},{"cell_type":"code","execution_count":8,"metadata":{"id":"0tIb56UcLUpX","executionInfo":{"status":"ok","timestamp":1705232327589,"user_tz":-60,"elapsed":2158,"user":{"displayName":"paolo","userId":"03714737150223700499"}}},"outputs":[],"source":["## NO NEED TO RUN THIS CELL\n","from src.data.collator import CustomDataCollator\n","\n","# data collator for Masked Language Model\n","data_collator_for_mlm = CustomDataCollator(tokenizer=tokenizer,\n","                                           mlm=True,\n","                                           mlm_probability=0.15)\n","# data collator for Regression task\n","data_collator_for_regression = CustomDataCollator(tokenizer=tokenizer,\n","                                                  mlm=False)"]},{"cell_type":"markdown","source":["---\n","# **BERT PARAMETERS**"],"metadata":{"id":"uHm0PUtnCr6M"}},{"cell_type":"markdown","source":[">## **Bert Custom Config**\n",">\n","> The *CustomBertConfig* class is an extension of the BertConfig class, specifically designed for handling tabular and time series data.\n",">\n","> 1. **Number of Columns (ncols)**: Specifies the number of columns in the tabular data, aligning with the number of input indices in one row.\n",">\n","> 2. **Vocabulary Size (vocab_size)**: Number of unique tokens in the data.\n",">\n","> 3. **Field Hidden Size (field_hidden_size)**: Sets the hidden size for field embeddings.\n",">\n","> 4. **Hidden Size (hidden_size)**: Determines the dimensionality of the encoder output.\n",">\n","> 5. **Number of Hidden Layers (num_hidden_layers)**: Defines the depth of the Transformer encoder.\n",">\n","> 6. **Number of Attention Heads (num_attention_heads)**: The number of attention mechanisms in each encoder layer.\n",">\n","> 7. **Pad Token ID (pad_token_id)**:  Represents the index used for padding.\n",">\n","> 8. **Masked Language Model Probability (mlm_probability)**: Ratio of tokens to mask for masked language modeling.\n",">\n","> **_Important_**:\n",">\n","> **Run the cell below to create a CustomBertConfig object. This is for demonstration purposes only as the config object will be created in the training manager given the dictionary of parameters.**\n"],"metadata":{"id":"O9Lpqw6M1-FH"}},{"cell_type":"code","execution_count":9,"metadata":{"id":"5hMWnY8ZHRZc","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1705232329429,"user_tz":-60,"elapsed":1845,"user":{"displayName":"paolo","userId":"03714737150223700499"}},"outputId":"7a7fdd10-bea6-4554-a876-3fcf3c6435bc"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["CustomBertConfig {\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"classifier_dropout\": null,\n","  \"field_hidden_size\": 64,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-12,\n","  \"max_position_embeddings\": 512,\n","  \"mlm_probability\": 0.15,\n","  \"model_type\": \"bert\",\n","  \"ncols\": 12,\n","  \"num_attention_heads\": 8,\n","  \"num_hidden_layers\": 6,\n","  \"pad_token_id\": 2,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.35.2\",\n","  \"type_vocab_size\": 2,\n","  \"use_cache\": true,\n","  \"vocab_size\": 429\n","}"]},"metadata":{},"execution_count":9}],"source":["## RUN THIS CELL - NOTHING TO CHANGE\n","from src.models.config import CustomBertConfig\n","\n","model_config_values = {\"ncols\": train_dataset.get_ncols(),\n","                       \"vocab_size\": len(vocab),\n","                       \"field_hidden_size\": 64,\n","                       \"hidden_size\": 64*train_dataset.get_ncols(),\n","                       \"num_hidden_layers\": 6,\n","                       \"num_attention_heads\": 8,\n","                       \"pad_token_id\": vocab.get_id(vocab.pad_token, vocab.special_tag),\n","                       \"mlm_probability\": 0.15}\n","config =  CustomBertConfig(**model_config_values)\n","config"]},{"cell_type":"markdown","metadata":{"id":"pXNxUKDHISdx"},"source":["---\n","# **THE MODEL**\n"]},{"cell_type":"code","execution_count":10,"metadata":{"id":"GvxY4lCQsG6G","executionInfo":{"status":"ok","timestamp":1705232329429,"user_tz":-60,"elapsed":3,"user":{"displayName":"paolo","userId":"03714737150223700499"}}},"outputs":[],"source":["## RUN THIS CELL - NO CHANGES NEEDED\n","from src.utils import set_seed\n","\n","# setting the seed for reproducibility\n","set_seed(2024)"]},{"cell_type":"markdown","metadata":{"id":"AstX1Vg0pero"},"source":["\n","> ## **Hierarchical Bert Language Model**\n",">\n","> This module contains the implementation of the Hierarchical Bert Language Model.\n","> It can be used for Masked Language Modeling and regression tasks. It represents the core component of our project.\n",">\n","> The model is composed of three components:\n","> - **TabRowEmbeddings**:\n","    - An embedding layer for tabular data.\n","    - It is designed to handle tabular data where each row consists of multiple columns.\n","    - Each individual token is mapped to an embedding. The sequence is then passed to a transformer encoder to capture relationships between columns.\n","    - A final linear layer transform the embeddings to the desired hidden size.\n","    - Thus, each row in a sample, will be represented by an embedding of dimension 'hidden_size'\n",">\n","> - **BertModel**:\n","    - A BertModel from the HuggingFace library.\n","    - It is used to capture relationships between rows.\n",">\n","> - **MLM-specific layers or Regression-specific layers**:\n","    - The MLM layers are used for Masked Language Modeling. They are used when pretraining the model to obtain a representation of the field tokens.\n","    - The regression layers are used for the regression task. They are used to fine-tune the model after pretraining.\n",">\n","> The forward step of the model is different for MLM and regression tasks:\n","> - **Masked Language Modeling**:\n","    - The input ids are passed through the TabRowEmbeddings layer to obtain the embeddings of the tabular data.\n","    - The 'sequence_length' embeddings are then passed to the BertModel.\n","    - The outputs of the BertModel are passed through the MLM layers to obtain the predictions at field level.\n","    - The predictions are compared to the masked LM labels at field level and the cross entropy loss is computed.\n",">\n","> - **Regression**:  \n","    - The input ids are passed through the TabRowEmbeddings layer to obtain the embeddings of the tabular data.\n","    - The embeddings are then passed to the BertModel.\n","    - The outputs of the BertModel are passed through the regression layers to obtain the predictions.\n","    - The predictions are compared to the regression targets (two targets) and the mean squared error loss is computed.\n",">\n","> **_Important_**:\n",">\n","> **The following code is not meant to be executed. It is only used to show how to instantiate the model.\n","> The models will be instantiated in the training manager based on the mode (mlm or regression).**  \n","    \n"]},{"cell_type":"code","execution_count":11,"metadata":{"id":"y7VJZ2y_juOm","executionInfo":{"status":"ok","timestamp":1705232333140,"user_tz":-60,"elapsed":3714,"user":{"displayName":"paolo","userId":"03714737150223700499"}}},"outputs":[],"source":["## NO NEED TO RUN THIS CELL\n","from src.models.hierarchical import HierarchicalBertLM\n","\n","model_mlm = HierarchicalBertLM(config=config,\n","                               vocab=vocab,\n","                               mode='mlm')\n","\n","model_regression = HierarchicalBertLM(config=config,\n","                                      vocab=vocab,\n","                                      mode='regression')"]},{"cell_type":"markdown","source":["---\n","# **TRAINING AND EVALUATION**"],"metadata":{"id":"0VJ8R6M1s4XV"}},{"cell_type":"markdown","source":["> ## **Weights & Biases**\n",">\n","> In order to log the training process and the metrics, we will use [Weights & Biases](https://wandb.ai/site).\n",">\n","> **_Important_:**\n",">\n","> **You can create a free account and login from the notebook running the following cell.**\n",">\n","> **While running the cell, you will be prompted to enter your API key. You can find your API key [here](https://wandb.ai/authorize).**\n","\n"],"metadata":{"id":"L86QZKH4gs8u"}},{"cell_type":"code","source":["## RUN THIS CELL - NO CHANGES NEEDED\n","import wandb\n","\n","wandb.login()"],"metadata":{"id":"qbuRUGWWsxfF"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["> ## **MLM Training Configuration**\n",">\n","> In this cell we define a dictionary containing the training parameters to train the Masked Language Model.\n",">\n","> The parameters are passed to the TrainingArguments class from the transformers library, that is used to instantiate the Trainer class. Thus, be sure that the parameters are valid for the TrainingArguments class. A check is performed in the TrainingManager class but it is better to check them before.\n",">\n","> **_Important_:**\n",">\n","> **Note if you're not planning to train the model, you can skip this cell. Pretrained models will be loaded in the next sections.**\n"],"metadata":{"id":"WaDwLWPoPow7"}},{"cell_type":"code","source":["## NO NEED TO RUN THIS CELL UNLESS YOU WANT TO TRAIN THE MODEL\n","training_config_dict = {\n","    'per_device_train_batch_size': 256,\n","    'per_device_eval_batch_size': 256,\n","    'num_train_epochs': 50,\n","    'logging_strategy': 'steps',\n","    'logging_first_step': True,\n","    'logging_steps': 1,\n","    'save_strategy': 'steps',\n","    'save_steps': 150,\n","    'evaluation_strategy': 'steps',\n","    'eval_steps': 150,\n","    'load_best_model_at_end': True,\n","    'disable_tqdm': False,\n","    'seed': 2024,\n","    'learning_rate': 1e-4,\n","    'lr_scheduler_type': 'constant',\n","    'report_to':'wandb'}"],"metadata":{"id":"-aAjL2Ps9Aas","executionInfo":{"status":"ok","timestamp":1705232342066,"user_tz":-60,"elapsed":3,"user":{"displayName":"paolo","userId":"03714737150223700499"}}},"execution_count":13,"outputs":[]},{"cell_type":"markdown","source":["> ## **Training Manager Class**\n",">\n","> The TrainingManager class is responsible for setting up the model, the data collator, the training arguments, and the HuggingFace Trainer.\n",">\n","> - **Class Initialization:**\n",">     - To initialize the class, we need to provide the model configuration dictionary, the training configuration dictionary, the training, validation, and test sets, the root directory, the project name, the model name, the mode (either 'mlm' or 'regression'), and the path to the pretrained model checkpoint (only required for 'regression' mode).\n",">     - The model configuration dictionary contains the model parameters to be logged.\n",">     - The training configuration dictionary contains the training parameters for the TrainingArguments class from HuggingFace. Be sure to check the [documentation](https://huggingface.co/transformers/main_classes/trainer.html#trainingarguments) for the list of available parameters. A check is performed to ensure that only valid parameters are provided.\n",">     - The training, validation, and test sets are instances of the PRSADataset class.\n",">     - The root directory is the directory where the output and logs directories will be created.\n",">     - The project name is the name of the project for logging on wandb.\n",">     - The model name is the name of the model for logging on wandb.\n",">     - The mode is either 'mlm' or 'regression'.\n",">     - The pretrained model path is the path to the pretrained model checkpoint.\n",">\n","> - **Directories and Logging:**\n",">     - The setup_directories() method sets up the checkpoints and logs directories.\n",">     - The setup_wandb() method sets up wandb for logging.\n",">\n","> - **Tokenizer and Collator:**\n",">     - The setup_tokenizer() method sets up the tokenizer needed in the data collator.\n",">     - The setup_collator() method sets up the data collator for training. If the mode is 'mlm', the data collator returns the labels for masked language modeling. If the mode is 'regression', the data collator returns the targets for regression.\n",">\n","> - **Model:**\n",">     - The setup_model() method sets up the model for training. If the mode a pretrained model path is provided, the model is initialized from the pretrained checkpoint. If the mode is 'mlm', the model is trained with masked language modeling. If the mode is 'regression', the model is initialized from the pretrained checkpoint (after MLM training) and trained for regression. The model is frozen except for the last two layers.\n",">\n","> - **Training:**\n",">     - The setup_training() method sets up the training arguments and trainer.\n",">     - The train() method must be called to train the model.\n",">     - The evaluate() method can be used to evaluate the model on the validation or test set.\n",">"],"metadata":{"id":"Nn4xhisRRwwN"}},{"cell_type":"markdown","source":["\n","> ## **MLM Training**\n",">\n","> **_Important_:**\n",">\n","> **Run this cell if you want to initialize the TrainingManager class for MLM training and run the train() method.**\n",">\n","> **A pretrained model checkpoint is provided to evaluate the model on the val/test set. If you prefer to train the model from scratch, set the pretrained_model_path to None.**\n"],"metadata":{"id":"Pg28JkAYCfiz"}},{"cell_type":"code","source":["## RUN THIS CELL TO SET UP THE TRAINING MANAGER FOR MLM - NO CHANGES NEEDED\n","from src.train.manager import TrainingManager\n","\n","## SET TO NONE TO TRAIN THE MODEL FROM SCRATCH\n","mlm_pretrained_model_path =  os.path.join(ROOT_DIR, 'output/mlm/checkpoints/prsa-model-1/checkpoint-final')\n","\n","training_manager_mlm = TrainingManager(model_config_dict=model_config_values,\n","                                       training_config_dict=training_config_dict,\n","                                       train_set=train_dataset,\n","                                       val_set=val_dataset,\n","                                       test_set=test_dataset,\n","                                       root_dir=ROOT_DIR,\n","                                       project_name='PRSATabBert',\n","                                       model_name='prsa-model-professor-test',\n","                                       mode='mlm',\n","                                       pretrained_model_path=mlm_pretrained_model_path)"],"metadata":{"id":"LhTzrVAU80gK"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["## NO NEED TO RUN THIS CELL\n","training_manager_mlm.train()"],"metadata":{"id":"JFAe8Ao4AbCp"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["> ## **MLM Evaluation**\n",">\n","> The following cell evaluates the model on the validation set.\n","> It returns the Cross Entropy Loss for MLM.\n",">\n","> **_Important_:**\n",">\n","> **If you run the .train() cell without finishing the training process, make sure to run again the training manager cell with the provided checkpoint.**"],"metadata":{"id":"V7IphHSkDGCj"}},{"cell_type":"code","source":["## RUN THIS CELL\n","training_manager_mlm.evaluate()"],"metadata":{"id":"goNKrYHXAR3G","colab":{"base_uri":"https://localhost:8080/","height":144},"executionInfo":{"status":"ok","timestamp":1705232364275,"user_tz":-60,"elapsed":6104,"user":{"displayName":"paolo","userId":"03714737150223700499"}},"outputId":"a3f1b5ff-557b-4918-a51d-bc3607fe89cb"},"execution_count":15,"outputs":[{"output_type":"stream","name":"stderr","text":["You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='33' max='33' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [33/33 00:04]\n","    </div>\n","    "]},"metadata":{}},{"output_type":"execute_result","data":{"text/plain":["{'eval_loss': 12.478158950805664,\n"," 'eval_runtime': 6.4304,\n"," 'eval_samples_per_second': 1306.3,\n"," 'eval_steps_per_second': 5.132}"]},"metadata":{},"execution_count":15}]},{"cell_type":"markdown","source":["> ## **Regression Training Configuration**\n",">\n","> In this cell we define a dictionary containing the training parameters to train the model for the regression task.\n",">\n","> The parameters are passed to the TrainingArguments class from the transformers library, that is used to instantiate the Trainer class. Thus, be sure that the parameters are valid for the TrainingArguments class. A check is performed in the TrainingManager class but it is better to check them before.\n",">\n","> **_Important_:**\n",">\n","> **Run this cell as it is.**\n"],"metadata":{"id":"2LsFQxvAgFS6"}},{"cell_type":"code","source":["## RUN THIS CELL - NO CHANGES NEEDED\n","training_config_dict = {\n","    'per_device_train_batch_size': 512,\n","    'per_device_eval_batch_size': 512,\n","    'num_train_epochs': 100,\n","    'logging_strategy': 'steps',\n","    'logging_first_step': True,\n","    'logging_steps': 1,\n","    'save_strategy': 'steps',\n","    'save_steps': 100,\n","    'evaluation_strategy': 'steps',\n","    'eval_steps': 100,\n","    'load_best_model_at_end': True,\n","    'disable_tqdm': False,\n","    'seed': 2024,\n","    'learning_rate': 1e-4,\n","    'lr_scheduler_type': 'linear',\n","    'report_to':'wandb'}"],"metadata":{"id":"Ofq7ElIS3DjW","executionInfo":{"status":"ok","timestamp":1705232364275,"user_tz":-60,"elapsed":10,"user":{"displayName":"paolo","userId":"03714737150223700499"}}},"execution_count":16,"outputs":[]},{"cell_type":"markdown","source":["\n","> ## **Regression Training**\n",">\n","> **_Important_:**\n",">\n","> **Run this cell if you want to initialize the TrainingManager class for regression training and run the train() method.**\n",">\n","> **A pretrained model checkpoint is provided to evaluate the model on the val/test set. If you prefer to train the model from scratch (starting from pretrained mlm), set the pretrained_model_path to mlm_pretrained_model_path.**\n"],"metadata":{"id":"V4K462mYC11n"}},{"cell_type":"code","source":["## RUN THIS CELL TO SET UP THE TRAINING MANAGER FOR REGRESSION - NO CHANGES NEEDED\n","from src.train.manager import TrainingManager\n","\n","## SET TO 'mlm_pretrained_model_path' TO TRAIN THE MODEL FROM THE PRETRAINED BERT (from mlm)\n","reg_pretrained_model_path = os.path.join(ROOT_DIR, 'output/regression/checkpoints/prsa-model-1-reg/checkpoint-final')\n","\n","training_manager_reg = TrainingManager(model_config_dict=model_config_values,\n","                                       training_config_dict=training_config_dict,\n","                                       train_set=train_dataset,\n","                                       val_set=val_dataset,\n","                                       test_set=test_dataset,\n","                                       root_dir=ROOT_DIR,\n","                                       model_name='prsa-reg-professor-test',\n","                                       mode='regression',\n","                                       pretrained_model_path=reg_pretrained_model_path)"],"metadata":{"id":"bO4U_c-wI1mv"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["## NO NEED TO RUN THIS CELL\n","training_manager_reg.train()"],"metadata":{"id":"lae5JPCeNdps"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["> ## **Regression Evaluation**\n",">\n","> The following cell evaluates the model on the validation/test set based on the value of test.\n","> The method evaluate returns the metrics, the predictions and the labels.\n",">\n","> The following metrics are computed:\n","> - **RMSE (Root Mean Squared Error)**: Measures the quadratic mean of the difference between the predicted and the actual values.\n","> - **MAE (Mean Absolute Error)**: Measures the mean of the absolute difference between the predicted and the actual values.\n","> - **R-Squared**: A statistical measure that represents the goodness of fit of a regression model.\n",">\n","> **_Important_:**\n",">\n","> **If you run the .train() cell without finishing the training process, make sure to run again the training manager cell with the provided checkpoint.**\n"],"metadata":{"id":"UEK4L4MfDOGx"}},{"cell_type":"code","source":["## RUN THIS CELL\n","## SET TEST TO FALSE/TRUE TO EVALUATE THE MODEL ON THE VAL/TEST SET\n","training_manager_reg.evaluate(test=True)"],"metadata":{"id":"Q1tvFM4MQHri","colab":{"base_uri":"https://localhost:8080/","height":315},"executionInfo":{"status":"ok","timestamp":1705232397396,"user_tz":-60,"elapsed":4391,"user":{"displayName":"paolo","userId":"03714737150223700499"}},"outputId":"f8955ce9-087e-49e1-c699-32066ed697cf"},"execution_count":18,"outputs":[{"output_type":"stream","name":"stderr","text":["You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":[]},"metadata":{}},{"output_type":"execute_result","data":{"text/plain":["({'rmse': 50.781578, 'mae': 30.270535, 'r_squared': 0.7886199766742757},\n"," array([[120.41879 , 124.4578  ],\n","        [123.752716, 124.91998 ],\n","        [123.75899 , 123.92647 ],\n","        ...,\n","        [ 20.964413,  30.007421],\n","        [ 25.65062 ,  37.11924 ],\n","        [ 28.10255 ,  41.86554 ]], dtype=float32),\n"," array([[ 88., 122.],\n","        [ 91., 117.],\n","        [ 96., 110.],\n","        ...,\n","        [ 24.,  53.],\n","        [ 37.,  68.],\n","        [ 50.,  63.]], dtype=float32))"]},"metadata":{},"execution_count":18}]},{"cell_type":"markdown","source":["---\n","# **WANDB**\n","\n","* [Link to wandb project](https://wandb.ai/neural-network-tab-bert/PRSATabBert?workspace=user-ferretti-2039579).\n","\n","* MLM task reports including training/evaluation plots can be seen at the following [link](https://api.wandb.ai/links/neural-network-tab-bert/g3x9vuqu).\n","\n","* Regression task reports including training/evaluation plots can be seen at the following [link](https://api.wandb.ai/links/neural-network-tab-bert/g0oldvef).\n","\n","\n","\n"],"metadata":{"id":"U98_vAZayBDw"}},{"cell_type":"markdown","source":["---\n","# **REFERENCES**\n","\n","- Inkit Padhi, Yair Schiff, Igor Melnyk, Mattia Rigotti, Youssef Mroueh, Pierre Dognin, Jerret Ross, Ravi Nair, and Erik Altman. \"Tabular Transformers for Modeling Multivariate Time Series\". 2021. arXiv:2011.01843 [cs.LG].\n","\n"],"metadata":{"id":"PHlbS-JiIXsD"}}],"metadata":{"accelerator":"GPU","colab":{"provenance":[{"file_id":"1up72qIv8AUCnTsZ4G31kcf0RER8KMRE1","timestamp":1697119356337}]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}