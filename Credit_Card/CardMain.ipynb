{"cells":[{"cell_type":"markdown","metadata":{"id":"fF7Q2RzLGBmX"},"source":["**NOTES**\n","\n","- **Comments with two hash symbols (##) are notes for whoever runs this notebook. They are cell-by-cell instruction of what to modify and what to keep as it is.**\n","\n","- **To avoid out-of-memory issues, it is strongly recommended not to run unneeded cells of code. Some of them are reported for demonstration purposes only and does not need to be run. Please follow the running instructions carefully. Running additional or unnecessary code can lead to excessive memory usage, causing Colab to disconnect.**"]},{"cell_type":"markdown","metadata":{"id":"Jznx-UrDgGn6"},"source":["---\n","# **IMPORT AND DRIVE MOUNTING**"]},{"cell_type":"markdown","metadata":{"id":"80-zCWd6pyXy"},"source":["This section is responsible for importing the necessary libraries and mounting the Google Drive if you run the notebook in Colab. It ensures that the required dependencies are available and the notebook can access the dataset from Google Drive."]},{"cell_type":"markdown","metadata":{"id":"lNENLLhXtA7d"},"source":["> ## **Drive Mounting and CWD**\n",">\n","> **_Important:_**  \n",">\n","> **If the \"Tabular_Transformer\" folder is a shared folder, you will need to create a shortcut to it in your own Drive. You can do this by navigating to the shared folder, right-clicking, and selecting \"Add shortcut to Drive\". Once you add the shortcut to your Drive, you should be able to access it from Colab as described below. Be sure to set the correct ROOT_DIR path.**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XfyiEqRLEqtG"},"outputs":[],"source":["%%capture\n","## CREATE A SHORTCUT TO THE DRIVE AND RUN THIS CELL\n","import os\n","from google.colab import drive\n","drive.mount('/content/drive', force_remount=True)\n","\n","## ADJUST THE PATHS IF NEEDED\n","ROOT_DIR='/content/drive/MyDrive/Tabular_Transformer/Credit_Card'\n","RAW_DATA_DIR = os.path.join(ROOT_DIR, 'data/raw')\n","PROCESSED_DATA_DIR = os.path.join(ROOT_DIR, 'data/processed')\n","VOCAB_DIR = os.path.join(ROOT_DIR, 'vocab')\n","\n","# navigate to the root directory and run the setup.py file to install the required dependencies\n","os.chdir(ROOT_DIR)\n","!pip install -r requirements.txt"]},{"cell_type":"markdown","metadata":{"id":"I3cuxfkBInw4"},"source":[">## **Logging**\n",">\n","> This cell initializes a basic logging configuration to monitor the activities."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"b4Y5tvkyF6O8"},"outputs":[],"source":["## RUN THIS CELL\n","from src.utils import setup_logging\n","\n","logger = setup_logging()"]},{"cell_type":"markdown","metadata":{"id":"f192nVm2Fzax"},"source":["> ## **Imports**\n",">\n","> Run this cell to import the required dependencies."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":8875,"status":"ok","timestamp":1705233138475,"user":{"displayName":"domenico meconi","userId":"12240534909735751246"},"user_tz":-60},"id":"JLTP36XEwKFw","outputId":"a011caa9-2b1d-4210-83a9-ef4ac319d322"},"outputs":[{"name":"stderr","output_type":"stream","text":["2024-01-14 11:52:09,475 - INFO - numexpr.utils - NumExpr defaulting to 2 threads.\n"]}],"source":["## RUN THIS CELL - NO CHANGES NEEDED\n","import json\n","import pickle as pkl\n","import random\n","import inspect\n","from tqdm import tqdm\n","from pathlib import Path\n","from typing import Optional, Tuple, List, Any, Dict, Union\n","import wandb\n","\n","import numpy as np\n","import pandas as pd\n","import torch\n","import torch.nn as nn\n","from torch.nn import init\n","from torch.utils.data import Dataset\n","\n","from scipy import stats\n","from sklearn.preprocessing import MinMaxScaler\n","from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score\n","from imblearn.under_sampling import RandomUnderSampler\n","from imblearn.over_sampling import SMOTE\n","\n","from transformers import BertConfig, BertTokenizerFast, DataCollatorForLanguageModeling, BertModel, PreTrainedModel\n","from transformers import TrainingArguments, Trainer\n","from transformers.activations import ACT2FN"]},{"cell_type":"markdown","metadata":{"id":"x4oFf2QXNceY"},"source":["---\n","# **DATA EXTRACTION**\n"]},{"cell_type":"markdown","metadata":{"id":"H9iEENAfGnX-"},"source":["> ## **Data Extractor Class**\n",">\n","> The DataExtractor class is designed to extract and load the data. Here's a summary of what the class does:\n",">\n","> - **Class Initialization:**\n",">   - The constructor initializes the object with the directory path where the data is stored and the number of samples per file to extract (if the directory contains multiple files).\n",">   - The inputs are validated and the data is extracted.\n",">\n","> - **Data Extraction:**\n",">   - The *_extract_data* private method allows to extract the data from all the files in the data directory, loading everything into a single unified pandas DataFrame. The Credit Card Dataset used for this project is a public dataset with 24 million transactions from 20000\n","users. Each transaction (row) has 12 fields (columns) consisting of both continuous and discrete nominal attributes, such\n","as merchant name, merchant address, transaction amount, etc. [Padhi et al.,  2021]\n",">\n",">   - The *_load_from_csv* private method loads the csv data at the specified location into a pandas DataFrame. You can either specify the number of samples per file to extract or set it to None to extract all the samples.\n",">\n",">   - The *_merge_time_col* private method merge various time-related columns into a single 'TIMESTAMP' column. This process ensures a unified and consistent time representation, which is crucial for time-series analysis.\n",">\n",">   - The *_split_data* private method splits data based on Station and TIMESTAMP columns. The data is splitted into training, validation, and test sets to avoid data leakage between the sets during preprocessing.\n","\n","\n","[Padhi et al.,  2021]: https://arxiv.org/abs/2011.01843 \"Padhi, I., Schiff, Y., Melnyk, I., et al. (2021). Tabular Transformers for Modeling Multivariate Time Series. arXiv:2011.01843 [cs.LG]\""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"G9amUxxZvDsn"},"outputs":[],"source":["class DataExtractor:\n","\n","    def __init__(self,\n","                 data_root_dir: str,\n","                 samples_per_file: Optional[int]=None,\n","                 train_size: Optional[float]=0.8,\n","                 val_size: Optional[float]=0.1) -> None:\n","        \"\"\"\n","        Initialize the DataExtractor module.\n","\n","        Args:\n","            - data_root_dir (str):\n","                The path to the directory containing the data.\n","            - samples_per_file (int, optional):\n","                The number of samples to extract from each file in the data directory. Default to None (all samples).\n","            - train_size (float, optional):\n","                The percentage of the data to be used for training. Default to 0.8.\n","            - val_size (float, optional):\n","                The percentage of the data to be used for validation, the remaining data will be used for testing. Default to 0.1.\n","        \"\"\"\n","        logger.info('Initializing the DataExtractor...')\n","        self.time_cols = ['Year', 'Month', 'Day', 'Time']\n","        # helper function to validate the initial inputs\n","        self._validate_initial_inputs(data_root_dir,\n","                                      samples_per_file,\n","                                      train_size,\n","                                      val_size)\n","        self.data_root_dir = data_root_dir\n","        self.samples_per_file = samples_per_file\n","        self.train_size = train_size\n","        self.val_size = val_size\n","        # extract the data into a pandas dataframe\n","        self._extract_data()\n","        logger.info(\"DataExtractor successfully initialized.\\n\")\n","\n","    def _validate_initial_inputs(self,\n","                                 data_root_dir: str,\n","                                 samples_per_file: Optional[int]=None,\n","                                 train_size: Optional[float]=0.8,\n","                                 val_size: Optional[float]=0.1) -> None:\n","        \"\"\"Helper function to validate the initial inputs.\"\"\"\n","\n","        if not isinstance(data_root_dir, str) or not os.path.isdir(data_root_dir):\n","            raise ValueError(f'\"data_root_dir\" must be a valid directory path. Got {data_root_dir}')\n","\n","        if samples_per_file is not None:\n","            if not isinstance(samples_per_file, int) or samples_per_file <= 0:\n","                raise ValueError(f'\"samples_per_file\" must be a positive integer or None. Got {samples_per_file}')\n","\n","        if not isinstance(train_size, float) or not 0 <= train_size <= 1:\n","            raise ValueError(f'\"train_size\" must be a float between 0 and 1. Got {train_size}')\n","\n","        if not isinstance(val_size, float) or not 0 <= val_size <= 1:\n","            raise ValueError(f'\"val_size\" must be a float between 0 and 1. Got {val_size}')\n","\n","        if train_size + val_size > 1:\n","            raise ValueError(f'The sum of \"train_size\" and \"val_size\" must be less than or equal to 1.')\n","\n","    def _merge_time_col(self,\n","                        data: pd.DataFrame) -> pd.DataFrame:\n","        \"\"\"\n","        Merging the 'year', 'month', 'day', 'time' columns in a unique column named TIMESTAMP.\n","        The method to_datetime convert in Unix timestamps in nanoseconds.\n","        Args:\n","            - data (pd.DataFrame):\n","                Pandas DataFrame containing the data.\n","\n","        Returns:\n","            - data (pd.DataFrame):\n","                Pandas DataFrame containing the data with a single 'TIMESTAMP' column containing the Unix timestamps.\n","        \"\"\"\n","        splittedTime = data['Time'].str.split(':', expand=True)\n","        data['TIMESTAMP'] = pd.to_datetime(\n","            dict(year = data['Year'],\n","                month = data['Month'],\n","                day = data['Day'],\n","                hour = splittedTime[0],\n","                minute = splittedTime[1])).astype(int)\n","        # use the MinMaxScaler to transform the time-related columns\n","        scaler = MinMaxScaler()\n","        data['TIMESTAMP'] = scaler.fit_transform(data['TIMESTAMP'].values.reshape(-1, 1))\n","        # drop the time-related columns\n","        data.drop(columns=self.time_cols, inplace=True)\n","        return data\n","\n","    def _load_from_csv(self,\n","                       file_path: str) -> pd.DataFrame:\n","        \"\"\"\n","        Load a DataFrame from a csv file.\n","\n","        Args:\n","            - file_path (str):\n","                The path to the csv file to be read.\n","\n","        Returns:\n","            - pd.DataFrame:\n","                Pandas DataFrame containing the data.\n","        \"\"\"\n","        # error check for the file_path argument\n","        if not isinstance(file_path, str):\n","            raise TypeError('file_path must be a string.')\n","        # get the dataframe from csv file\n","        if os.path.exists(file_path):\n","            data = pd.read_csv(file_path, nrows=self.samples_per_file)\n","            # check if the dataframe is empty\n","            if data.empty:\n","                raise ValueError('Data cannot be empty.')\n","            # check if the dataframe contains all the required columns\n","            if not all(col in data.columns for col in self.time_cols):\n","                missing_cols = [col for col in self.time_cols if col not in data.columns]\n","                raise ValueError(f\"The following columns are missing from the dataframe: {', '.join(missing_cols)}\")\n","            return data\n","        else:\n","            raise FileNotFoundError(f'The file at the provided path {os.path.split(file_path)[-1]} was not found.\\n')\n","\n","    def _split_data(self,\n","                    data: pd.DataFrame) -> Tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame]:\n","        \"\"\"\n","        Split the data into train, validation, and test sets based on timestamp.\n","\n","        Args:\n","            - data (pd.DataFrame):\n","                Pandas DataFrame containing the data.\n","            - train_size (float):\n","                Proportion of the dataset to include in the train split (0 to 1).\n","            - val_size (float):\n","                Proportion of the dataset to include in the validation split (0 to 1).\n","\n","        Returns:\n","            - train_data (pd.DataFrame):\n","                Pandas DataFrame containing the training data.\n","            - val_data (pd.DataFrame):\n","                Pandas DataFrame containing the validation data.\n","            - test_data (pd.DataFrame):\n","                Pandas DataFrame containing the test data.\n","        \"\"\"\n","        # sort the data by timestamp\n","        data = data.sort_values(by='TIMESTAMP')\n","        # split the data into train, validation, and test sets\n","        train_end = int(len(data) * self.train_size)\n","        val_end = train_end + int(len(data) * self.val_size)\n","        train_data = data.iloc[:train_end]\n","        val_data = data.iloc[train_end:val_end]\n","        test_data = data.iloc[val_end:]\n","        return train_data, val_data, test_data\n","\n","    def _extract_data(self) -> None:\n","        \"\"\"\n","        Extract the data from the provided directory.\n","        If multiple files are present, they are merged into a single Pandas DataFrame.\n","        The data is then split into train, validation, and test sets.\n","        \"\"\"\n","        # files inside the data directory\n","        all_files = [os.path.join(self.data_root_dir, file) for file in os.listdir(self.data_root_dir) if os.path.isfile(os.path.join(self.data_root_dir, file))]\n","        # merge all the files into a DataFrame\n","        train_dataframes = []\n","        val_dataframes = []\n","        test_dataframes = []\n","        for file_path in tqdm(all_files, desc='Data Extraction:'):\n","            # load the data from the csv file\n","            data = self._load_from_csv(file_path)\n","            # merge the time-related columns\n","            data = self._merge_time_col(data)\n","            # split the data into train, validation, and test sets\n","            train_data, val_data, test_data = self._split_data(data)\n","            train_dataframes.append(train_data)\n","            val_dataframes.append(val_data)\n","            test_dataframes.append(test_data)\n","        # concatenate all the dataframes\n","        self.train_data = pd.concat(train_dataframes, ignore_index=True)\n","        self.val_data = pd.concat(val_dataframes, ignore_index=True)\n","        self.test_data = pd.concat(test_dataframes, ignore_index=True)\n","        logger.info(\n","            f'Successfully extracted {len(all_files)} DataFrame. '\n","            f'Train DataFrame has {len(self.train_data)} rows. '\n","            f'Validation DataFrame has {len(self.val_data)} rows. '\n","            f'Test DataFrame has {len(self.test_data)} rows.\\n')"]},{"cell_type":"markdown","metadata":{"id":"WFpeLFSRM-Gr"},"source":["> **_Important_:**\n",">\n","> **In the next section, the preprocessed data will be loaded to simplify the run of the notebook, thus there's no need to run the code cell below.**\n",">\n","> **However, feel free to review the code below to understand how to use the DataExtractor class, but running it is not required and can lead to memory issues!**\n",">\n","> **If you wish to run the code below, either load only a subset of the data (we used 5,000,000 samples per file) or restart the kernel before running the next section.**\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1881,"status":"ok","timestamp":1705233140353,"user":{"displayName":"domenico meconi","userId":"12240534909735751246"},"user_tz":-60},"id":"zzwh2rzbIyGw","outputId":"83a7c308-4c46-422a-e8c7-1d21ea7c8800"},"outputs":[{"name":"stderr","output_type":"stream","text":["2024-01-14 11:52:17,188 - INFO - root - Initializing the DataExtractor...\n","Data Extraction:: 100%|██████████| 1/1 [00:01<00:00,  1.68s/it]\n","2024-01-14 11:52:19,091 - INFO - root - Successfully extracted 1 DataFrame. Train DataFrame has 8000 rows. Validation DataFrame has 1000 rows. Test DataFrame has 1000 rows.\n","\n","2024-01-14 11:52:19,094 - INFO - root - DataExtractor successfully initialized.\n","\n"]}],"source":["## NO NEED TO RUN THIS CELL\n","samples_per_file = 10000\n","\n","extractor = DataExtractor(data_root_dir=RAW_DATA_DIR,\n","                          samples_per_file=samples_per_file,\n","                          train_size=0.8,\n","                          val_size=0.1)\n","train_data = extractor.train_data\n","val_data = extractor.val_data\n","test_data = extractor.test_data"]},{"cell_type":"markdown","metadata":{"id":"wc_PufOvIo2G"},"source":["---\n","# **VOCABULARY**"]},{"cell_type":"markdown","metadata":{"id":"1TEL51zHSHej"},"source":["> ## **Vocabulary Class**\n",">\n","> The *Vocab* class is designed to manage, create, save and load vocabularies.\n",">\n","> - **Class Initialization**:\n","    - The class starts by defining a set of custom special tokens as class-level constants.\n","    - The vocab object accepts columns from the DataFrame to create the vocabulary, the actual data, a directory to save the vocabulary, and target columns.\n","    - Various attributes are first initialized and validated to ensure they are correctly provided.\n","    - Special tokens are initialized and added to the vocabulary.\n","    - Vocabularies are created based on the provided data.\n",">\n","> - **Vocabulary Creation**:\n","    - The vocabulary is constructed using the unique values from the provided columns in the data. Each unique value is added to the vocabulary with a corresponding tag (the field name), global id (the index considering all the tokens of the vocabulary) and local id (the index considering only the tokens in the current field).\n","    - Vocabulary structure: {column tag: {token: [global_id, local_id]}}\n","    - id2token structure: {global_id: [token, tag, local_id]}\n",">    \n","> - **Utility Methods**:\n","    - The class also provides a method to retrieve the global id of a token, a method to retrieve the token corresponding to a global id, a method to map between global and local ids using a lookup tensor and two methods to save/load the vocabulary.\n",">   \n","> - **Vocabulary Summary Display**:\n","    - The Vocab class includes the print_vocab_summary method. This method allows for a detailed display of various statistics and characteristics of the vocabulary. It's possible to print special tokens, sample tokens from each column, the size of the vocabulary for each column, the data types of each column, and the total length of the vocabulary. You can specify the sample size and token limit per column.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"G2qG5GBZyIhb"},"outputs":[],"source":["class Vocab:\n","\n","    CLS_TOKEN = '[CLS]'\n","    START_TOKEN = '[START]'\n","    END_TOKEN = '[END]'\n","    UNK_TOKEN = '[UNK]'\n","    SEP_TOKEN = '[SEP]'\n","    MASK_TOKEN = '[MASK]'\n","    PAD_TOKEN = '[PAD]'\n","\n","    def __init__(self,\n","                 data: pd.DataFrame=None,\n","                 cols_for_vocab: List[str]=None) -> None:\n","        \"\"\"\n","        Initialize the vocabulary with special tokens and the provided columns and data.\n","\n","        Args:\n","            - cols_for_vocab (List[str]):\n","                A list of columns used to create the vocabularies.\n","            - data (pd.DataFrame):\n","                The Pandas DataFrame containing the data for vocabulary creation.\n","        \"\"\"\n","        logger.info('Initializing the vocabulary...')\n","\n","        # initialize dataset attributes and attribute validation\n","        self.cols_for_vocab = cols_for_vocab\n","        self.data = data\n","        self.token2id = {}\n","        self.id2token = {}\n","        # validate the input attributes\n","        self._validate_attributes()\n","        # initialize the vocabularies with the special tokens\n","        self._initialize_special_tokens()\n","        # fill the token2id and id2token mappings and create a lookup table from global_ids to local_ids\n","        self._create_vocabularies()\n","        self.lookup_tensor = self._create_lookup_tensor()\n","\n","        logger.info('Vocabularies successfully created.\\n')\n","\n","    def _validate_attributes(self) -> None:\n","        \"\"\"Helper function to validate the class attributes.\"\"\"\n","        # checks for the columns and the data\n","        if self.cols_for_vocab is None or len(self.cols_for_vocab) == 0:\n","            raise ValueError('\"cols_for_vocab\" cannot be None or empty.')\n","        if self.data is None or self.data.empty:\n","            raise ValueError('Data cannot be None or empty.')\n","        # checks for the columns in the data\n","        if not all(col in self.data.columns for col in self.cols_for_vocab):\n","            missing_cols = [col for col in self.cols_for_vocab if col not in self.data.columns]\n","            raise ValueError(f\"The following columns are missing from the dataframe: {', '.join(missing_cols)}\")\n","\n","    def _add_tokens_to_vocab(self,\n","                             tokens: List[Any],\n","                             tag: str) -> None:\n","        \"\"\"\n","        Add tokens from a given field to the vocabularies.\n","\n","        Args:\n","            - tokens (List[Any]):\n","                The list of tokens to add to the vocabulary.\n","            - tag (str):\n","                The tag or category for the tokens.\n","        \"\"\"\n","        # fill in the values into token2id and id2token mappings\n","        self.token2id[tag] = {}\n","        global_index = len(self.id2token)\n","        # structure of token2id: {tag: {token: [global_index, local_index]}}\n","        # structure of id2token: {global_index: [token, tag, local_index]}\n","        for local_index, token in enumerate(tokens):\n","            self.token2id[tag][token] = [global_index, local_index]\n","            self.id2token[global_index] = [token, tag, local_index]\n","            global_index += 1\n","\n","    def _initialize_special_tokens(self) -> None:\n","        \"\"\"Initialize special tokens, token2id and id2token vocabularies.\"\"\"\n","        # store the special tokens\n","        self.cls_token = Vocab.CLS_TOKEN\n","        self.start_token = Vocab.START_TOKEN\n","        self.end_token = Vocab.END_TOKEN\n","        self.unk_token = Vocab.UNK_TOKEN\n","        self.sep_token = Vocab.SEP_TOKEN\n","        self.mask_token = Vocab.MASK_TOKEN\n","        self.pad_token = Vocab.PAD_TOKEN\n","        # add the special tokens to the token2id and id2token vocabularies\n","        self.special_tokens = [self.unk_token, self.sep_token, self.pad_token,\n","                               self.cls_token, self.mask_token, self.start_token, self.end_token]\n","        self.special_tag = 'SPECIAL'\n","        self._add_tokens_to_vocab(self.special_tokens, self.special_tag)\n","\n","    def _create_vocabularies(self) -> None:\n","        \"\"\"Create token2id and id2token vocabularies based on the provided columns (fields) and data.\"\"\"\n","        # for each column extract the unique values and build the mappings\n","        for col in tqdm(self.cols_for_vocab, desc='Creating vocabularies...'):\n","            if col not in self.data.columns:\n","                raise ValueError(f\"Column {col} not found in data.\")\n","            unique_values = self.data[col].unique()\n","            self._add_tokens_to_vocab(unique_values, col)\n","\n","    def _create_lookup_tensor(self) -> torch.Tensor:\n","        \"\"\"\n","        Create a lookup tensor to map global ids to local ids.\n","        It constructs a tensor where each index corresponds to a global id and its value is the local id.\n","\n","        Returns:\n","            - torch.Tensor: A tensor where each index is a global id and its value is the corresponding local id.\n","        \"\"\"\n","        max_global_id = max(self.id2token.keys())\n","        lookup_tensor = torch.full((max_global_id + 1,), -100, dtype=torch.long)\n","        for global_id, value in self.id2token.items():\n","            lookup_tensor[global_id] = value[2]\n","        return lookup_tensor\n","\n","    def get_id(self,\n","               token: Any,\n","               tag: str) -> int:\n","        \"\"\"\n","        Retrieve the global index for a token and tag.\n","\n","        Args:\n","            - token (Any):\n","                The token to find.\n","            - tag (str):\n","                The tag or category for the token.\n","\n","        Returns:\n","            - int:\n","                The global index of the token.\n","        \"\"\"\n","        # get the vocabulary corresponding to the given field (tag) and from that retrieve the global index of the token\n","        return self.token2id.get(tag, {}).get(token, self.token2id[self.special_tag][self.unk_token])[0]\n","\n","    def get_token(self,\n","                  id: int) -> str:\n","        \"\"\"\n","        Retrieve the token corresponding to an index.\n","\n","        Args:\n","            - id (int):\n","                The index of the token.\n","\n","        Returns:\n","            - str:\n","                The token corresponding to that global index.\n","        \"\"\"\n","        return self.id2token.get(id, [self.unk_token])[0]\n","\n","    def get_special_tokens(self) -> Dict[str, str]:\n","        \"\"\"\n","        Create the mapping between custom tokens and the standard keys used in BERT-like tokenizers.\n","        Inspiration was taken by: https://github.com/IBM/TabFormer/blob/main/dataset/vocab.py\n","\n","        Returns:\n","            - special_tokens_map (Dict[str, str]):\n","                The dictionary mapping the custom tokens to the standard keys used in Bert Tokenizer\n","        \"\"\"\n","        special_tokens_map = {}\n","        # create a mapping between custom special tokens and standard keys in BERT tokenizer\n","        keys = [\"unk_token\", \"sep_token\", \"pad_token\", \"cls_token\", \"mask_token\", \"bos_token\", \"eos_token\"]\n","        for key, token in zip(keys, self.special_tokens):\n","            token = \"%s_%s\" % (self.special_tag, token)\n","            special_tokens_map[key] = token\n","\n","        return special_tokens_map\n","\n","    def get_global_ids(self,\n","                       field_name: str) -> List[int]:\n","        \"\"\"\n","        Get the indeces of the tabular dataset columns.\n","\n","        Args:\n","            - field_name (str):\n","                Name of the field.\n","        Returns:\n","            - List[int]:\n","                List containing the token ids in a given field.\n","        \"\"\"\n","        field_global_ids = [self.token2id[field_name][idx][0] for idx in self.token2id[field_name]]\n","        return field_global_ids\n","\n","    def map_global_to_local(self,\n","                            global_ids: torch.Tensor) -> torch.Tensor:\n","        \"\"\"\n","        Map the global ids to the corresponding field local ids using the lookup tensor. This method is intended to be used during training.\n","\n","        Args:\n","            - global_ids (torch.Tensor):\n","                A tensor containing token global ids.\n","\n","        Returns:\n","            - local_ids (torch.Tensor):\n","                A tensor containing token local ids corresponding to the token global ids.\n","        \"\"\"\n","        lookup_tensor_device = self.lookup_tensor.to(global_ids.device)\n","        local_ids = lookup_tensor_device[global_ids]\n","        local_ids.masked_fill_(global_ids == -100, -100)\n","        return local_ids\n","\n","    def print_vocab_summary(self,\n","                            print_special_tokens:bool=True,\n","                            print_sample_tokens:bool=True,\n","                            sample_size:int=5,\n","                            token_limit_per_column:int=5,\n","                            print_vocab_size_per_column:bool=True,\n","                            print_column_data_types:bool=True,\n","                            print_vocab_length:bool=True) -> None:\n","        \"\"\"\n","        Print a summary of the vocabulary based on the provided flags.\n","\n","        Args:\n","            print_special_tokens (bool): Whether to print the special tokens. Default to True.\n","            print_sample_tokens (bool): Whether to print sample tokens from each column. Default to True.\n","            sample_size (int): Number of columns to sample from. Default to 5.\n","            token_limit_per_column (int): Number of tokens to show per sampled column. Default to 5.\n","            print_vocab_size_per_column (bool): Whether to print the size of vocabulary per column. Default to True.\n","            print_column_data_types (bool): Whether to print the data types of each column. Default to True.\n","            print_vocab_length (bool): Whether to print the total length of the vocabulary. Default to True.\n","        \"\"\"\n","        if print_special_tokens:\n","            print(f'Special tokens: {self.special_tokens}\\n')\n","        if print_sample_tokens:\n","            print(\"Sampling from the Vocabulary:\\n\")\n","            vocab_sample = dict(random.sample(list(self.token2id.items()), sample_size))\n","            for column, tokens in vocab_sample.items():\n","                print(f\"COLUMN_TAG: {column}\")\n","                print(\"TOKEN: [GLOBAL IDX, LOCAL IDX]\")\n","                limited_tokens = list(tokens.items())[:token_limit_per_column]\n","                for token, indices in limited_tokens:\n","                    print(f\"{token}: {indices}\")\n","                print(\"\\n\")\n","        if print_vocab_size_per_column:\n","            for col in self.cols_for_vocab:\n","                print(f\"Number of tokens in column '{col}': {len(self.token2id[col])}\")\n","            print(\"\\n\")\n","        if print_column_data_types:\n","            print(\"Data Types per Column:\")\n","            for col in self.cols_for_vocab:\n","                print(f\"Column '{col}': {self.data[col].dtype}\")\n","            print(\"\\n\")\n","        if print_vocab_length:\n","            print(f\"Total Length of the Vocabulary: {len(self)}\")\n","\n","    def save_vocab(self,\n","                   vocab_dir: str) -> None:\n","        \"\"\"\n","        Save the vocabularies at the specified path in two formats:\n","            - One compatible with BERT tokenizer\n","            - One to have easy access to the vocabulary object when loading it for the validation and test set\n","        Inspiration was taken by: https://github.com/IBM/TabFormer/blob/main/dataset/vocab.py\n","\n","        Args:\n","            - vocab_dir (str):\n","                The directory where to save the vocabularies.\n","        \"\"\"\n","        logger.info('Saving vocabularies...')\n","        if not isinstance(vocab_dir, str):\n","            raise TypeError(f'\"vocab_dir\" must be a string. Got {type(vocab_dir)}')\n","        # create the directory where to store the vocabularies\n","        if not os.path.exists(vocab_dir):\n","            os.makedirs(vocab_dir)\n","        self.vocab_file_for_bert = os.path.join(vocab_dir, f'vocab.nb')\n","        vocab_object_file_pickle = os.path.join(vocab_dir, f'vocab.pkl')\n","        # save the vocabularies in a format compatible with BERT tokenizer\n","        with open(self.vocab_file_for_bert, \"w\") as fout:\n","            for idx in self.id2token:\n","                token, field, _ = self.id2token[idx]\n","                token = \"%s_%s\" % (field, token)\n","                fout.write(\"%s\\n\" % token)\n","        # save the vocabulary object to have easy access to the vocabulary when loading it for the validation and test set\n","        with open(vocab_object_file_pickle, 'wb') as f:\n","            pkl.dump(self, f)\n","        logger.info('Vocabularies successfully saved.\\n')\n","\n","    @staticmethod\n","    def load_vocab(vocab_dir: str) -> 'Vocab':\n","        \"\"\"\n","        Load the vocabulary object from the specified path.\n","\n","        Args:\n","            - vocab_dir (str):\n","                The directory where the vocabularies are stored.\n","\n","        Returns:\n","            - Vocab:\n","                The vocabulary object.\n","        \"\"\"\n","        logger.info('Loading vocabularies...')\n","        if not isinstance(vocab_dir, str):\n","            raise TypeError(f'\"vocab_dir\" must be a string. Got {type(vocab_dir)}')\n","        filename = os.path.join(vocab_dir, 'vocab.pkl')\n","        if not os.path.exists(filename):\n","            raise FileNotFoundError(f\"Vocab file not found in {vocab_dir}\")\n","        # load the vocabulary object\n","        with open(filename, 'rb') as f:\n","            vocab = pkl.load(f)\n","        logger.info('Vocab object successfully loaded.\\n')\n","        return vocab\n","\n","    def __len__(self) -> int:\n","        \"\"\"\n","        Return the length of the vocabulary.\n","\n","        Returns:\n","            - int:\n","                The length of the vocabulary.\n","        \"\"\"\n","        return len(self.id2token)\n"]},{"cell_type":"markdown","metadata":{"id":"Zx_RWpB-Ac0r"},"source":["---\n","# **THE DATASET**"]},{"cell_type":"markdown","metadata":{"id":"PSoe-cunRT7_"},"source":["> ## **TransactionDataset Class**\n",">\n","> The TransactionDataset class is designed to take raw data and prepare it for train and test phases.\n",">\n","> - **Class Initialization**:\n","    - Initializes with a DataFrame and the dataset mode (train, train-cls, val, test).\n","    When the mode is 'train', the vocabulary is saved in the specified directory and transformations are applied based on statistical information from the training data.\n","    When the mode is 'val' or 'test', the vocabulary is loaded from the specified directory and transformations are applied based on the statistical information from the training data, same for 'train-cls' with the addition of dataset balancing using SMOTE, that generates synthetic instances of the minority class.\n","    - Other arguments include: directory paths where to save the vocabulary and the class instance, the columns to discretize, the columns to drop, the target columns, the sequence length and the stride.\n","    - The attributes are validated to ensure that the class is initialized correctly.\n","    - The data is preprocessed based on the mode.\n","    - The data is tokenized using the vocabulary.\n","    - The samples and targets are prepared for time-series analysis based on the sequence length and the stride.\n",">\n","> - **Data Preprocessing**:\n","    - Processes the input data based on the specified mode (train, val, test).\n","    - In training mode, the data is discretized and a vocabulary is created and saved.\n","    - In validation and test modes the data is discretized based on the bin edges found with the training data, then the vocabulary created with the training data is loaded and applied to the val/test data.\n",">\n","> - **Discretization**:\n","    - Compute the number of bins to discretize the dataset based on its interquartile range (IQR).\n","    - The method uses the Freedman-Diaconis Rule to compute the width of each bin. The rule is robust to outliers and is given as  $$ \\text{Bin width} = \\frac{2 \\times \\text{IQR}}{\\sqrt[3]{\\text{num observations}}} $$    \n","    - The number of bins is calculated as\n","$$ \\text{Number of bins} = \\frac{\\text{max value} - \\text{min value}}{\\text{Bin width}} $$\n","    - The bin labels are computed based on the number of bins specified.\n","    - The data is discretized by assigning each value to the closest bin label.\n","    - The bin labels are saved in a json file and loaded when the mode is 'val' or 'test'.\n",">\n","> - **Vocabulary**:\n","    - Uses the *Vocab* class to manage the vocabulary of the dataset.\n","    - The vocabulary is created based on the columns specified in *cols_for_vocab*.\n","    - Each column has its own vocabulary, with the following structure: {token: [global_index, local_index]}\n","    - The vocabulary is created and saved when the mode is 'train', otherwise it is loaded from the specified directory.\n",">\n","> - **Tokenization**:\n",">   - Maps the tokens in the columns specified in *cols_for_vocab* to the corresponding global indices.\n","> - **Sample Preparation**:\n","    - Structures the data into samples and targets with a format suitable for time-series analysis.\n","    - A single sample contains (seq_len+1)*(ncols+1) token ids. The shape of the sample is (seq_len+1, ncols+1). The +1 comes in the first case from the classification token and in the second from the padding token, which are necessary for BERT training.\n","    - The number of samples obtained in the end depends on the stride and on the number of subsequent rows considered for each sample (sequence length).\n","\n",">\n","> - **Saving and Loading the Dataset**:\n","    - It's possible to save the entire class instance using pickle for efficient storage and retrieval.\n","    - It's possible to load the class instance, a static method is provided for this purpose."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hZvn_a4u8yhR"},"outputs":[],"source":["class TransactionDataset(Dataset):\n","\n","    COLS_TO_DISCRETIZE = ['Amount', 'TIMESTAMP']\n","    COLS_FOR_VOCAB = ['Card', 'TIMESTAMP', 'Amount', 'Use Chip', 'Merchant Name', 'Merchant City', 'Merchant State','Zip', 'MCC', 'Errors?']\n","    TARGET_COLS = [\"Is Fraud?\"]\n","\n","    def __init__(self,\n","                 data: pd.DataFrame,\n","                 mode: str='train',\n","                 vocab_dir: str='vocab',\n","                 save_dir: str='data/processed',\n","                 cols_to_discretize: Optional[List[str]]=None,\n","                 cols_for_vocab: Optional[List[str]]=None,\n","                 target_cols: Optional[List[str]]=None,\n","                 smote: Optional[bool]=False,\n","                 sequence_length: Optional[int]=10,\n","                 stride: Optional[int]=5) -> None:\n","        \"\"\"\n","        Initialize the PRSADataset module.\n","\n","        Args:\n","            - data (pd.DataFrame):\n","                The DataFrame containing the data.\n","            - mode (str, optional):\n","                The mode of the dataset. Can be 'train', 'train-cls', 'val' or 'test'. Default to 'train'.\n","            - 'vocab_dir' (str, optional):\n","                When mode is 'train', the vocabulary is saved in this directory.\n","                When mode is 'val' or 'test', the vocabulary is loaded from this directory. Default to 'vocab'.\n","            - save_dir (str, optional):\n","                The directory where to save the class instance. Default to 'data/processed'.\n","            - cols_to_discretize (List[str], optional):\n","                List of columns to discretize. If not provided, defaults to class-level constant ['SO2', 'NO2', 'CO', 'O3', 'TEMP', 'PRES', 'DEWP', 'WSPM', 'RAIN', 'TIMESTAMP'].\n","            - cols_for_vocab (List[str], optional):\n","                List of columns to be used for the vocabulary. If not provided, defaults to class-level constant ['SO2', 'NO2', 'CO', 'O3', 'TEMP', 'PRES', 'DEWP', 'WSPM', 'RAIN', 'TIMESTAMP', 'wd'].\n","            - target_cols (List[str], optional):\n","                List of columns to be used as targets. If not provided, defaults to class-level constant ['PM2.5', 'PM10'].\n","            - smote (bool, optional):\n","                Whether to apply SMOTE to the dataset. Default to False.\n","            - sequence_length (int, optional):\n","                The numbers of subsequent row to consider as a sequence. Default to 10.\n","            - stride (int, optional):\n","                The step of the sliding window when combining subsequent rows. Default to 5.\n","        \"\"\"\n","\n","        logger.info('Initializing the TransactionDataset...')\n","        # initialize dataset attributes\n","        self.data = data.copy()\n","        self.mode = mode\n","        self.vocab_dir = vocab_dir\n","        # initialize the columns to discretize, to drop and the target columns\n","        self.cols_to_discretize = cols_to_discretize or TransactionDataset.COLS_TO_DISCRETIZE.copy()\n","        self.cols_for_vocab = cols_for_vocab or TransactionDataset.COLS_FOR_VOCAB.copy()\n","        self.target_cols = target_cols or TransactionDataset.TARGET_COLS.copy()\n","        self.smote = smote\n","        # initialize attributes for the sequences\n","        self.sequence_length = sequence_length\n","        self.stride = stride\n","        self.samples, self.targets = [], []\n","        # attributes validation\n","        self._validate_attributes()\n","        self._fill_na()\n","        self._preprocess_data(mode=self.mode)\n","        self._tokenize_data()\n","        self._prepare_samples()\n","        if self.smote:\n","            self.apply_smote()\n","        self.save(save_dir)\n","        logger.info('TransactionDataset successfully initialized.\\n')\n","\n","    def _validate_attributes(self) -> None:\n","        \"\"\"Helper function to validate the attributes of the class.\"\"\"\n","        # handle the validation of attributes using tuple (variable, expected type, error message)\n","        validations = [\n","            (self.data, pd.DataFrame, '\"data\" must be a pandas DataFrame'),\n","            (self.mode, str, '\"mode\" must be a string'),\n","            (self.vocab_dir, str, '\"vocab_dir\" must be a string'),\n","            (self.smote, bool, '\"smote\" must be a boolean'),\n","            (self.sequence_length, int, '\"sequence_length\" must be an integer'),\n","            (self.stride, int, '\"stride\" must be an integer'),\n","            (self.cols_to_discretize, (list, type(None)), '\"cols_to_discretize\" must be a list or None'),\n","            (self.target_cols, (list, type(None)), '\"target_cols\" must be a list or None')]\n","        # check if the mode is valid\n","        if self.mode not in ['train', 'train-cls','val', 'test']:\n","            raise ValueError(f'Invalid mode: {self.mode}. Must be one of \"train\", \"val\" or \"test\".')\n","        # iterate over the list of tuples (variable, expected type, error message)\n","        for var, expected_type, error_msg in validations:\n","            if not isinstance(var, expected_type):\n","                raise TypeError(f'{error_msg}. Got {type(var)}')\n","        # check positivity of samples_per_file, sequence_length and stride\n","        if self.sequence_length <= 0 or self.stride <= 0:\n","            raise ValueError(f'\"sequence_length\" and \"stride\" must be positive integers.')\n","        # check that the data is not None or empty\n","        if self.data is None or self.data.empty:\n","            raise ValueError('Data cannot be None or empty.')\n","        # check if the necessary columns are in the DataFrame\n","        required_cols = set(self.cols_to_discretize + self.target_cols + self.cols_for_vocab + self.target_cols)\n","        if not all(col in self.data.columns for col in required_cols):\n","            missing_cols = [col for col in required_cols if col not in self.data.columns]\n","            raise ValueError(f\"The following columns are missing from the dataframe: {', '.join(missing_cols)}\")\n","\n","    def _fill_na(self) -> None:\n","        \"\"\"\n","        Handle the Nan values of the dataset\n","        'Zip' Nan -> 0\n","        'Errors?' Nan -> None\n","        'Merchant State' Nan -> None\n","        \"\"\"\n","        logger.info(\"Removing Nan values from the dataset...\")\n","        self.data['Zip'] = self.data['Zip'].fillna(0)\n","        self.data['Errors?'] = self.data['Errors?'].fillna('None')\n","        self.data['Merchant State'] = self.data['Merchant State'].fillna('None')\n","        logger.info(\"Nan values succesfully removed\")\n","\n","    def _compute_number_bins(self,\n","                             col_data: pd.Series) -> int:\n","        \"\"\"\n","        Compute the number of bins to discretize a dataset based on its interquartile range (IQR).\n","        The method uses the Freedman-Diaconis Rule to compute the width of each bin.\n","        The rule is robust to outliers and is given as 2*IQR/cubic_root(num_observations).\n","        The number of bins is calculated as (max_value - min_value)/bin_width.\n","\n","        Args:\n","            - col_data (pd.Series):\n","                The data series to be discretized.\n","\n","        Returns:\n","            - int:\n","                The number of bins to be used for discretization.\n","        \"\"\"\n","        IQR = stats.iqr(col_data, rng=(25,75), nan_policy='omit')\n","        bin_width = 2 * IQR / np.cbrt(len(col_data.unique()))\n","        range = np.max(col_data) - np.min(col_data)\n","        n_bins = int(range/bin_width)\n","        return n_bins\n","\n","    def _compute_bin_labels(self,\n","                            col_data: pd.Series,\n","                            n_bins: int) -> np.ndarray:\n","        \"\"\"\n","        Compute the bin labels based on the number of bins specified.\n","        The labels serve as the edges for each bin.\n","\n","        Args:\n","            - col_data (pd.Series):\n","                The data series for which the bin labels are to be computed.\n","            - n_bins (int):\n","                The number of bins for quantile calculation.\n","\n","        Returns:\n","            - np.ndarray:\n","                The unique bin labels, which are the edges for each bin.\n","        \"\"\"\n","        quantiles = np.linspace(0, 1, n_bins + 1)\n","        bin_labels = np.quantile(col_data, quantiles)\n","        bin_labels = np.unique(bin_labels)\n","        return bin_labels\n","\n","    def _discretize_column(self,\n","                           col: str,\n","                           bin_labels: Optional[np.ndarray]=None) -> np.ndarray:\n","        \"\"\"\n","        Helper function to discretize a single column.\n","        If bin_labels is not provided, the number of bins and bin labels are computed.\n","\n","        Args:\n","            - col (str):\n","                The column to be discretized.\n","            - bin_labels (np.ndarray, optional):\n","                The bin labels to be used for discretization. If not provided, they are computed.\n","\n","        Returns:\n","            - np.ndarray:\n","                The bin labels used for discretization.\n","        \"\"\"\n","        # compute the number of bins and the bin labels\n","        if bin_labels is None:\n","            n_bins = self._compute_number_bins(self.data[col])\n","            bin_labels = self._compute_bin_labels(self.data[col], n_bins)\n","        # subtract the value with the closest bin label\n","        self.data[col] = self.data[col].apply(lambda x: bin_labels[np.argmin(np.abs(bin_labels - x))])\n","        return bin_labels\n","\n","    def _encode_amount(self) -> None:\n","        \"\"\"Encode the currency string into float without $\"\"\"\n","        self.data['Amount'] = self.data['Amount'].apply(lambda x: float(x.replace('$', '')))\n","\n","    def _encode_fraud(self) -> None:\n","        \"\"\"\n","        Encode the Yes/No into 1/0\n","        \"\"\"\n","        self.data['Is Fraud?'] = (self.data['Is Fraud?'] == 'Yes').astype(int)\n","\n","    def _discretize_data(self,\n","                         save_stats: bool=True) -> None:\n","        \"Discretize the data. The columns specified in self.cols_to_discretize are discretized based on the Freedman-Diaconis Rule.\"\n","        logger.info('Starting the discretization process...')\n","        self._encode_amount()\n","        self._encode_fraud()\n","        # filling na values by interpolating\n","        self.data[self.cols_to_discretize] = self.data[self.cols_to_discretize].interpolate()\n","        # discretize each column\n","        if save_stats:\n","            info = {}\n","            for col in tqdm(self.cols_to_discretize, desc='Discretizing columns'):\n","                # discretize the column and save the bin labels\n","                info[col] = self._discretize_column(col).tolist()\n","            # save the bin labels in a json file\n","            with open('bin_stats.json', 'w') as f:\n","                json.dump(info, f)\n","        else:\n","            # load the bin labels from the json file\n","            with open('bin_stats.json', 'r') as f:\n","                info = json.load(f)\n","            for col in tqdm(self.cols_to_discretize, desc='Applying saved discretization'):\n","                self._discretize_column(col, np.array(info[col]))\n","        logger.info('Discretization process completed.\\n')\n","\n","    def _create_and_save_vocab(self) -> None:\n","        \"\"\"When mode is 'train', create and save the vocabulary in the specified directory.\"\"\"\n","        self.vocab = Vocab(data=self.data,\n","                           cols_for_vocab=self.cols_for_vocab)\n","        self.vocab.save_vocab(vocab_dir=self.vocab_dir)\n","\n","    def _preprocess_data(self,\n","                         mode: str) -> None:\n","        \"\"\"\n","        Preprocess the data based on the mode.\n","        - If mode is 'train', columns are dropped and the data is discretized. The vocabulary is created and saved.\n","        - If mode is 'val' or 'test', the vocabulary is loaded and the data is discretized.\n","        \"\"\"\n","        logger.info(f'Preprocessing the {mode} data...')\n","        if self.mode=='train' or self.mode=='train-cls':\n","            # data preprocessing (drop columns and discretize), then create and save the vocabulary\n","            self._discretize_data(save_stats=True)\n","            self._create_and_save_vocab()\n","        else:\n","            # apply the same preprocessing as in train mode, then load the vocabulary\n","            self._discretize_data(save_stats=False)\n","            try:\n","                self.vocab = Vocab.load_vocab(vocab_dir=self.vocab_dir)\n","            except FileNotFoundError:\n","                raise FileNotFoundError(f'Vocabulary not found in {self.vocab_dir}. Please run the script in train mode first.')\n","        logger.info(f'{mode} data successfully preprocessed.\\n')\n","\n","    def _tokenize_data(self) -> None:\n","        \"\"\"Map the tokens in \"cols_for_vocab\" to the corresponding indices.\"\"\"\n","        logger.info('Converting data to indices...')\n","        self.tokenized_data = self.data.copy()\n","        # apply the get_id function to each element of the dataframe to get the id\n","        for col in tqdm(self.cols_for_vocab, desc='Tokenizing columns...'):\n","            self.tokenized_data[col] = self.data[col].apply(lambda x: self.vocab.get_id(x, col))\n","        logger.info('Tokenization process completed.\\n')\n","\n","    def _prepare_samples(self) -> None:\n","        \"\"\"\n","        Structuring the samples and the targets for Time-Series Analysis.\n","        A single sample contains seq_len+1*ncols token ids, representing a sequence of registrations in the tabular data.\n","        The number of samples obtained in the end depends on the stride and on the number of subsequent rows considered for each sample.\n","        \"\"\"\n","        logger.info('Preparing samples and targets...')\n","        sep_id = self.vocab.get_id(self.vocab.sep_token, self.vocab.special_tag)\n","        # get the column indices\n","        feature_col_indices = [self.tokenized_data.columns.get_loc(c) for c in self.cols_for_vocab]\n","        target_cols_indices = [self.tokenized_data.columns.get_loc(c) for c in self.target_cols]\n","        data_numpy = self.tokenized_data.to_numpy()\n","        # group by User and iterate through groups to prepare the samples\n","        groups = self.tokenized_data.groupby('User')\n","        for _, group_indices in tqdm(groups.groups.items(), desc='Preparing Samples'):\n","            user_data = data_numpy[group_indices]\n","            nrows = len(user_data) - self.sequence_length\n","            for start_id in range(0, nrows, self.stride):\n","                sample, target = [], []\n","                slice_start = start_id\n","                slice_end = start_id + self.sequence_length\n","                # get the values of the sample and the target\n","                sliced_data = user_data[slice_start:slice_end]\n","                sample_values = sliced_data[:, feature_col_indices]\n","                target_value = sliced_data[:, target_cols_indices]\n","                # add the sep token to the end of the sample\n","                sep_column = np.full((self.sequence_length, 1), sep_id)\n","                sample = np.hstack((sample_values, sep_column)).ravel()\n","                flat_target = [item for sublist in target_value.tolist() for item in sublist]\n","                # if there is at least one fraud in the target, the target is 1, otherwise it is 0\n","                if 1 in flat_target:\n","                    target = 1\n","                else:\n","                    target = 0\n","                # append the sample and the target to the list\n","                self.samples.append(sample)\n","                self.targets.append(target)\n","        logger.info('Samples and targets successfully organized.\\n')\n","\n","    def get_ncols(self) -> int:\n","        \"\"\"\n","        Retrieve the number of columns used for the vocabulary (+1 for the sep token).\n","\n","        Returns:\n","            -int:\n","                number of columns used for the vocabulary (+1 for the sep token).\n","        \"\"\"\n","        return len(self.cols_for_vocab) + 1\n","\n","\n","    def __len__(self) -> int:\n","        \"\"\"\n","        Retrieve the length of the dataset.\n","\n","        Returns:\n","            - int:\n","                The number of samples in the dataset.\n","        \"\"\"\n","        return len(self.samples)\n","\n","    def __getitem__(self,\n","                    index: int)-> Tuple[torch.Tensor, torch.Tensor]:\n","        \"\"\"\n","        Retrieve the sample and the target at the specified index.\n","\n","        Args:\n","            - index (int):\n","                The index of the sample.\n","\n","        Returns:\n","            - tuple: A Tuple containing:\n","                - 'sample': The tensor containing the sample values. The shape is (sequence_length+1, ncols) as the sample contains the row representing the cls token.\n","                - 'target': The tensor containing the target values.\n","        \"\"\"\n","        # create the cls row and add it to the sample\n","        cls_id =  self.vocab.get_id(self.vocab.cls_token, self.vocab.special_tag)\n","        cls_row = torch.full((1, self.get_ncols()), cls_id, dtype=torch.long)\n","        sample = torch.tensor(self.samples[index].tolist(), dtype=torch.long).reshape(self.sequence_length, -1)\n","        sample = torch.cat((cls_row, sample), dim=0)\n","        # get the target\n","        target = torch.tensor(self.targets[index], dtype=torch.float32)\n","        return sample, target\n","\n","    def apply_smote(self) -> None:\n","        \"\"\"Balance the data by applying SMOTE to samples and targets\"\"\"\n","        logger.info('Balancing the data...')\n","        # convert the samples and the targets to numpy arrays\n","        X_train_numpy = np.array(self.samples)\n","        y_train_numpy = np.array(self.targets)\n","        # apply SMOTE\n","        smote = SMOTE(sampling_strategy='minority', random_state=2024)\n","        X_train_resampled, y_train_resampled = smote.fit_resample(X_train_numpy, y_train_numpy)\n","        self.samples = X_train_resampled\n","        self.targets = y_train_resampled\n","        logger.info('Data successfully balanced.\\n')\n","\n","    def save(self,\n","             data_dir: str) -> None:\n","        \"\"\"\n","        Save the class instance to a file using pickle.\n","\n","        Args:\n","            - data_dir (str): The path where to save the class instance.\n","        \"\"\"\n","        logger.info('Saving Card Dataset...')\n","        if not isinstance(data_dir, str):\n","            raise TypeError(f'\"data_dir\" must be a string. Got {type(data_dir)}')\n","        # create the directory where to store the vocabularies\n","        if not os.path.exists(data_dir):\n","            os.makedirs(data_dir)\n","        self.card_file = os.path.join(data_dir, f'card_{self.mode}.pkl')\n","        with open(self.card_file, 'wb') as file:\n","            pkl.dump(self, file)\n","        logger.info(f'Class instance successfully saved.\\n')\n","\n","    @staticmethod\n","    def load(data_dir: str,\n","             mode: str) -> 'TransactionDataset':\n","        \"\"\"\n","        Load a class instance from a file using pickle.\n","\n","        Args:\n","            - data_dir (str): The path of the directory to load the class instance from.\n","\n","        Returns:\n","            - TransactionDataset: An instance of the TransactionDataset class.\n","        \"\"\"\n","        logger.info(f'Loading Card Dataset {mode} set...')\n","        if not isinstance(data_dir, str):\n","            raise TypeError(f'\"data_dir\" must be a string. Got {type(data_dir)}')\n","        # check if the mode is valid\n","        if mode not in ['train', 'train-cls', 'val', 'test']:\n","            raise ValueError(f'Invalid mode: {mode}. Must be one of \"train\", \"train-cls\", \"val\" or \"test\".')\n","        filename = os.path.join(data_dir, f'card_{mode}.pkl')\n","        if not os.path.exists(filename):\n","            raise FileNotFoundError(f\"Card file not found in {data_dir}\")\n","        with open(filename, 'rb') as file:\n","            card = pkl.load(file)\n","        logger.info(f'Class instance successfully loaded.\\n')\n","        return card"]},{"cell_type":"markdown","metadata":{"id":"2uX3KVWYVxem"},"source":["> **_Important_:**\n",">\n","> **Run the following cell keeping the 'load' argument to 'True' to load the preprocessed data we used for our experiments.\n","If you want to preprocess the data again, change the 'load' argument to 'False'. Keep in mind that this will take more time.**\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":72224,"status":"ok","timestamp":1705233213095,"user":{"displayName":"domenico meconi","userId":"12240534909735751246"},"user_tz":-60},"id":"bFBRWSPfVMsa","outputId":"c8817936-c36a-4777-f89b-aa184bac9762"},"outputs":[{"name":"stderr","output_type":"stream","text":["2024-01-14 11:52:19,352 - INFO - root - Loading Card Dataset train set...\n","2024-01-14 11:52:48,481 - INFO - root - Class instance successfully loaded.\n","\n","2024-01-14 11:52:48,487 - INFO - root - Loading Card Dataset train-cls set...\n","2024-01-14 11:53:09,060 - INFO - root - Class instance successfully loaded.\n","\n","2024-01-14 11:53:09,069 - INFO - root - Loading Card Dataset val set...\n","2024-01-14 11:53:19,030 - INFO - root - Class instance successfully loaded.\n","\n","2024-01-14 11:53:19,040 - INFO - root - Loading Card Dataset test set...\n","2024-01-14 11:53:31,661 - INFO - root - Class instance successfully loaded.\n","\n"]}],"source":["## RUN THIS CELL - NOTHING TO CHANGE\n","load = True\n","if not load:\n","    train_dataset = TransactionDataset(data=train_data,\n","                                       mode='train',\n","                                       vocab_dir=VOCAB_DIR,\n","                                       save_dir=PROCESSED_DATA_DIR)\n","    train_dataset_cls = TransactionDataset(data=train_data,\n","                                           mode='train-cls',\n","                                           vocab_dir=VOCAB_DIR,\n","                                           smote=True,\n","                                           stride=10,\n","                                           save_dir=PROCESSED_DATA_DIR)\n","    val_dataset = TransactionDataset(data=val_data,\n","                                     mode='val',\n","                                     vocab_dir=VOCAB_DIR,\n","                                     save_dir=PROCESSED_DATA_DIR)\n","    test_dataset = TransactionDataset(data=test_data,\n","                                      mode='test',\n","                                      vocab_dir=VOCAB_DIR,\n","                                      save_dir=PROCESSED_DATA_DIR)\n","else:\n","    train_dataset = TransactionDataset.load(data_dir=PROCESSED_DATA_DIR,\n","                                            mode='train')\n","    train_dataset_cls = TransactionDataset.load(data_dir=PROCESSED_DATA_DIR,\n","                                                mode='train-cls')\n","    val_dataset = TransactionDataset.load(data_dir=PROCESSED_DATA_DIR,\n","                                          mode='val')\n","    test_dataset = TransactionDataset.load(data_dir=PROCESSED_DATA_DIR,\n","                                           mode='test')"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":6,"status":"ok","timestamp":1705233213095,"user":{"displayName":"domenico meconi","userId":"12240534909735751246"},"user_tz":-60},"id":"36qG1KIW_v_O","outputId":"2350edfe-aa95-473d-c4ab-f60dbb040c00"},"outputs":[{"name":"stdout","output_type":"stream","text":["First sample:\n"," tensor([[    3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n","             3],\n","        [    8,    26,  1208,  2385,  6481, 37003, 43902, 47943, 62531, 62640,\n","             1],\n","        [    8,    26,  1675,  2385,  2805, 37004, 43902, 47944, 62536, 62640,\n","             1],\n","        [    8,    26,  1305,  2385,  2805, 37004, 43902, 47944, 62536, 62640,\n","             1],\n","        [    8,    26,   563,  2385,  2758, 37004, 43902, 47944, 62542, 62640,\n","             1],\n","        [    8,    26,   989,  2385,  2601, 37003, 43902, 47943, 62535, 62640,\n","             1],\n","        [    8,    26,  1424,  2385,  2410, 37004, 43902, 47947, 62548, 62640,\n","             1],\n","        [    8,    26,   703,  2385,  2805, 37004, 43902, 47944, 62536, 62640,\n","             1],\n","        [    8,    26,  1609,  2385,  2805, 37004, 43902, 47944, 62536, 62640,\n","             1],\n","        [    8,    26,  2096,  2385,  2805, 37004, 43902, 47944, 62536, 62640,\n","             1],\n","        [    8,    26,  2140,  2385,  6503, 37003, 43902, 47943, 62533, 62640,\n","             1]])\n","Shape [seq_len+1, num_cols+1]: torch.Size([11, 11])\n","\n","Target associated to the first sample:\n"," 0.0\n","\n"]}],"source":["## RUN THIS CELL - NOTHING TO CHANGE\n","# -----------------------------------------------------------------------------------------------------------\n","# NOTE: This cell shows the structure of a single sample from the CardDataset. Each sample comprises multiple rows,\n","# each with 10 columns. The data in these columns has been discretized, mapped to the nearest bin edge, and\n","# then converted to indices. The separator index has beeen added.\n","# Observing this sample helps in understanding the preprocessing steps applied to the\n","# dataset, such as discretization and tokenization, and how the data is presented to the data collator.\n","# -----------------------------------------------------------------------------------------------------------\n","first_sample = train_dataset[0][0]\n","first_sample_label = train_dataset[0][1]\n","print(f\"First sample:\\n {first_sample}\")\n","print(f\"Shape [seq_len+1, num_cols+1]: {first_sample.shape}\\n\")\n","print(f\"Target associated to the first sample:\\n {first_sample_label}\\n\")"]},{"cell_type":"markdown","metadata":{"id":"PniJy44b_6Hh"},"source":["> ## **Vocabulary Extraction and Summary**\n",">\n","> **_Important_**:\n",">\n","> **Run the following cell to extract the vocabulary from the training dataset and to display a summary of it.**"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5,"status":"ok","timestamp":1705233213095,"user":{"displayName":"domenico meconi","userId":"12240534909735751246"},"user_tz":-60},"id":"_mN6tyed1MQK","outputId":"f8dbdeec-ec14-4c0a-e834-4c7784107131"},"outputs":[{"name":"stdout","output_type":"stream","text":["Special tokens: ['[UNK]', '[SEP]', '[PAD]', '[CLS]', '[MASK]', '[START]', '[END]']\n","\n","Sampling from the Vocabulary:\n","\n","COLUMN_TAG: Errors?\n","TOKEN: [GLOBAL IDX, LOCAL IDX]\n","None: [62640, 0]\n","Technical Glitch,: [62641, 1]\n","Insufficient Balance,: [62642, 2]\n","Bad PIN,: [62643, 3]\n","Bad CVV,: [62644, 4]\n","\n","\n","COLUMN_TAG: Use Chip\n","TOKEN: [GLOBAL IDX, LOCAL IDX]\n","Swipe Transaction: [2385, 0]\n","Online Transaction: [2386, 1]\n","Chip Transaction: [2387, 2]\n","\n","\n","COLUMN_TAG: Merchant City\n","TOKEN: [GLOBAL IDX, LOCAL IDX]\n","Quincy: [34574, 0]\n","Panama City: [34575, 1]\n","Lincoln Park: [34576, 2]\n","Brandon: [34577, 3]\n","Saint Marks: [34578, 4]\n","\n","\n","COLUMN_TAG: Zip\n","TOKEN: [GLOBAL IDX, LOCAL IDX]\n","32352.0: [44055, 0]\n","32401.0: [44056, 1]\n","48146.0: [44057, 2]\n","33511.0: [44058, 3]\n","32355.0: [44059, 4]\n","\n","\n","COLUMN_TAG: Card\n","TOKEN: [GLOBAL IDX, LOCAL IDX]\n","3: [7, 0]\n","0: [8, 1]\n","2: [9, 2]\n","4: [10, 3]\n","6: [11, 4]\n","\n","\n","Number of tokens in column 'Card': 8\n","Number of tokens in column 'TIMESTAMP': 255\n","Number of tokens in column 'Amount': 2115\n","Number of tokens in column 'Use Chip': 3\n","Number of tokens in column 'Merchant Name': 32186\n","Number of tokens in column 'Merchant City': 9324\n","Number of tokens in column 'Merchant State': 157\n","Number of tokens in column 'Zip': 18476\n","Number of tokens in column 'MCC': 109\n","Number of tokens in column 'Errors?': 22\n","\n","\n","Data Types per Column:\n","Column 'Card': int64\n","Column 'TIMESTAMP': float64\n","Column 'Amount': float64\n","Column 'Use Chip': object\n","Column 'Merchant Name': int64\n","Column 'Merchant City': object\n","Column 'Merchant State': object\n","Column 'Zip': float64\n","Column 'MCC': int64\n","Column 'Errors?': object\n","\n","\n","Total Length of the Vocabulary: 62662\n"]}],"source":["vocab = train_dataset.vocab\n","vocab.print_vocab_summary(print_special_tokens=True,\n","                          print_sample_tokens=True,\n","                          sample_size=5,\n","                          token_limit_per_column=5,\n","                          print_vocab_size_per_column=True,\n","                          print_column_data_types=True,\n","                          print_vocab_length=True)"]},{"cell_type":"markdown","metadata":{"id":"__7LvG_ta6Fi"},"source":["---\n","# **BERT PARAMETERS**"]},{"cell_type":"markdown","metadata":{"id":"O9Lpqw6M1-FH"},"source":[">## **Bert Custom Config**\n",">\n","> The *CustomBertConfig* class is an extension of the BertConfig class, specifically designed for handling tabular and time series data.\n",">\n","> 1. **Number of Columns (ncols)**: Specifies the number of columns in the tabular data, aligning with the number of input indices in one row.\n",">\n","> 2. **Vocabulary Size (vocab_size)**: Number of unique tokens in the data.\n",">\n","> 3. **Field Hidden Size (field_hidden_size)**: Sets the hidden size for field embeddings.\n",">\n","> 4. **Hidden Size (hidden_size)**: Determines the dimensionality of the encoder output.\n",">\n","> 5. **Number of Hidden Layers (num_hidden_layers)**: Defines the depth of the Transformer encoder.\n",">\n","> 6. **Number of Attention Heads (num_attention_heads)**: The number of attention mechanisms in each encoder layer.\n",">\n","> 7. **Pad Token ID (pad_token_id)**:  Represents the index used for padding.\n",">\n","> 8. **Masked Language Model Probability (mlm_probability)**: Ratio of tokens to mask for masked language modeling.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5hMWnY8ZHRZc"},"outputs":[],"source":["class CustomBertConfig(BertConfig):\n","    \"\"\"\n","    Custom config class for a hierarchal Bert Model for Tabular Data and Time Series analysis.\n","\n","    The BertConfig is the configuration class to store the configuration of a [`BertModel`].\n","\n","    Refer to the following link for source code and documentation of BertConfig:\n","        - https://github.com/huggingface/transformers/blob/v4.34.0/src/transformers/models/bert/configuration_bert.py#L72\n","    \"\"\"\n","\n","    def __init__(self,\n","                 ncols: Optional[int]=12,\n","                 vocab_size: Optional[int]=429,\n","                 field_hidden_size: Optional[int]=64,\n","                 hidden_size: Optional[int]=768,\n","                 num_hidden_layers: Optional[int]=6,\n","                 num_attention_heads: Optional[int]=8,\n","                 pad_token_id: Optional[int]=0,\n","                 mlm_probability: Optional[float]=0.15,\n","                 **kwargs) -> None:\n","\n","        \"\"\"\n","        Initialize the CustomBertConfig module.\n","\n","        Args:\n","            - ncols (int, optional):\n","                The number of columns in the tabular data. Correspond to the number of 'input_ids' in one row. Default to 12.\n","            - vocab_size (int, optional):\n","                Vocabulary size of the model. Defines the number of different tokens that can be represented by the `inputs_ids`. Default to 429.\n","            - field_hidden_size (int, optional):\n","                 Hidden size for field embeddings.. Default to 64.\n","            - hidden_size (int, optional):\n","                Dimensionality of the encoder layers and the pooler layer.\n","                Corresponds to the dimensionality of the row embedding. Default to 768.\n","            - num_hidden_layers (int, optional):\n","                Number of hidden layers in the Transformer encoder. Default to 6.\n","            - num_attention_heads (int, optional):\n","                Number of attention heads for each attention layer in the Transformer encoder. Default to 8.\n","            - pad_token_int (int, optional):\n","                Index used for padding. Default to 0.\n","            - mlm_probability (float, optional):\n","                Ratio of tokens to mask for masked language modeling. Default to 0.15.\n","        \"\"\"\n","\n","        super().__init__(pad_token_id=pad_token_id, **kwargs)\n","\n","        self.ncols = ncols\n","        self.field_hidden_size = field_hidden_size\n","        self.hidden_size = hidden_size\n","        self.vocab_size = vocab_size\n","        self.num_attention_heads = num_attention_heads\n","        self.pad_token_id = pad_token_id\n","        self.num_hidden_layers = num_hidden_layers\n","        self.mlm_probability = mlm_probability"]},{"cell_type":"markdown","metadata":{"id":"9RGchOgnYgvl"},"source":["> **_Important_**:\n",">\n","> **Run the cell below to create a CustomBertConfig object. This is for demonstration purposes only as the config object will be created in the training manager given the dictionary of parameters.**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"n3Sc-EO_YpxI"},"outputs":[],"source":["ncols=train_dataset.get_ncols()\n","\n","model_config_values = {\n","    \"vocab_size\": len(vocab),\n","    \"ncols\": ncols,\n","    \"field_hidden_size\": 128,\n","    \"hidden_size\": 128*ncols,\n","    \"num_hidden_layers\": 12,\n","    \"num_attention_heads\": ncols,\n","    \"pad_token_id\": vocab.get_id(vocab.pad_token, vocab.special_tag),\n","    \"sequence_length\": train_dataset.sequence_length,\n","    \"stride\": train_dataset.stride\n","    }\n","config =  CustomBertConfig(**model_config_values)"]},{"cell_type":"markdown","metadata":{"id":"6-uPFoOZfDBN"},"source":["---\n","# **TOKENIZER AND DATA COLLATOR**"]},{"cell_type":"markdown","metadata":{"id":"FHPOff5z7ZFX"},"source":["> ## **Tokenizer**\n",">\n","> In this cell we initialize the BERT tokenizer. The tokenizer is initialized with the vocabulary file from the vocab object.\n","This tokenizer is used in the CustomDataCollator class to pad the input ids and mask tokens for MLM tasks.\n",">\n","> **_Important_**:\n",">\n","> **Run the following cell to initialize the tokenizer.**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lW5O4HPSIu17"},"outputs":[],"source":["tokenizer = BertTokenizerFast(vocab_file=vocab.vocab_file_for_bert,\n","                              do_lowercase=False,\n","                              **vocab.get_special_tokens())"]},{"cell_type":"markdown","metadata":{"id":"W6J9_N8qBc7A"},"source":["> ## **Custom Data Collator**\n",">\n","> The *CustomDataCollator* class is designed to handle tabular and time series data, preparing the samples, the targets and the masked language model labels.\n",">\n","> - **Hugging Face Compatibility**:\n","    - It inherits from *DataCollatorForLanguageModeling*, ensuring compatibility with Hugging Face.\n","    - The class requires a Bert tokenizer for padding and masking the input ids.\n",">    \n","> - ***`__call__`* Method**:\n","    - Each row represent a collection of features (already converted to indices). Remember that the sequence lenght parameter of the dataset defines the number of subsequent rows that constitute a single sample.\n","    - The collator efficiently groups multiple samples into a batch, the batch received by the model has shape [batch, seq_len+1, ncols+1].\n","    - The method can handle both MLM and classification tasks.\n","    - In MLM mode, it masks certain tokens in the input ids based on mlm_probability and returns the labels.\n","    - In classification mode, it returns the targets.\n","> - For source code and usage, refer to the [Hugging Face's documentation](https://github.com/huggingface/transformers/blob/v4.34.0/src/transformers/data/data_collator.py#L607)."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"nQfuaV8hKOL2"},"outputs":[],"source":["class CustomDataCollator(DataCollatorForLanguageModeling):\n","    \"\"\"\n","    Custom Data Collator for Tabular Data and Time Series analysis.\n","\n","    This class inherits from DataCollatorForLanguageModeling from huggingface.\n","    It is designed to handle tabular and time series where each row consists of multiple columns and each sample consists of multiple rows.\n","    The collator can be used for Masked Language Modeling tasks.\n","\n","    Refer to the following link for source code and documentation of DataCollatorForLanguageModelling:\n","        - https://github.com/huggingface/transformers/blob/v4.34.0/src/transformers/data/data_collator.py#L607\n","    \"\"\"\n","\n","    def __call__(self,\n","                 samples: List[Tuple[torch.Tensor, torch.Tensor]]) -> Dict[str, torch.Tensor]:\n","        \"\"\"\n","        Collates the samples into a batch. It can handle both MLM and classification tasks.\n","        For MLM tasks, it masks certain tokens in the input ids based on mlm_probability and returns the labels.\n","        For classification tasks, it returns the labels.\n","\n","        Args:\n","            - samples (List[Dict[str, Tensor]]):\n","                List of dictionaries containing the samples and their labels.\n","\n","        Returns:\n","            - Dict[str, Tensor]:\n","                A dictionary containing input ids, attention mask and target (MLM labels or classification labels).\n","        \"\"\"\n","        # pad the batch\n","        input_ids = [sample[0] for sample in samples]\n","        targets = [sample[1] for sample in samples]\n","        batch = self.tokenizer.pad({\"input_ids\": input_ids}, return_tensors=\"pt\") # expected shape [batch, seq_len+1, ncols+1]\n","        if self.mlm:\n","            # get the shape of the input ids and flatten the samples to mask tokens for MLM\n","            sz = batch['input_ids'].shape\n","            input_ids = batch['input_ids'].view(sz[0], -1) # expected shape [batch, seq_len+1*ncols+1]\n","            # mask the tokens with a method from DataCollatorForLanguageModeling\n","            input_ids, labels = self.torch_mask_tokens(input_ids)\n","            # reconstruct the initial shape\n","            batch['input_ids'] = input_ids.view(sz)\n","            batch['labels'] = labels.view(sz)\n","        else:\n","            batch['labels'] = torch.stack(targets)\n","        return batch"]},{"cell_type":"markdown","metadata":{"id":"XVSvWIrwf-ya"},"source":["> **_Important_:**\n",">\n","> **In the following cell, we demonstrate how to use the CustomDataCollator class for MLM and classification tasks.\n","> Note that there's no need to create the collator object manually, as the CustomDataCollator object will be created in the training manager.**\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BmQ2nVA9gNH5"},"outputs":[],"source":["## NO NEED TO RUN THIS CELL\n","# data collator for Masked Language Model\n","data_collator_for_mlm = CustomDataCollator(tokenizer=tokenizer,\n","                                           mlm=True,\n","                                           mlm_probability=config.mlm_probability)\n","# data collator for Classification task\n","data_collator_for_regression = CustomDataCollator(tokenizer=tokenizer,\n","                                                  mlm=False)"]},{"cell_type":"markdown","metadata":{"id":"bISTym48YNVy"},"source":["---\n","# **THE MODEL**\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QdsHf3TMWRXe"},"outputs":[],"source":["## RUN THIS CELL - NO CHANGES NEEDED\n","from src.utils import set_seed\n","\n","# setting the seed for reproducibility\n","set_seed(2024)"]},{"cell_type":"markdown","metadata":{"id":"_VwD8WvUWTm5"},"source":["\n","> ## **Hierarchical Bert Language Model**\n",">\n","> This module contains the implementation of the Hierarchical Bert Language Model.\n","> It can be used for Masked Language Modeling and classification tasks. It represents the core component of our project.\n",">\n","> The model is composed of three components:\n","> - **TabRowEmbeddings**:\n","    - An embedding layer for tabular data.\n","    - It is designed to handle tabular data where each row consists of multiple columns.\n","    - Each individual token is mapped to an embedding. The sequence is then passed to a transformer encoder to capture relationships between columns.\n","    - A final linear layer transform the embeddings to the desired hidden size.\n","> - **BertModel**:\n","    - A BertModel from the HuggingFace library.\n","    - It is used to capture relationships between rows.\n","> - **MLM-specific layers or Classification-specific layers**:\n","    - The MLM layers are used for Masked Language Modeling. They are used when pretraining the model to obtain a representation of the input.\n","    - The classification layers are used for the classification task. They are used to fine-tune the model after pretraining.\n",">\n","> The forward step of the model is different for MLM and classification tasks:\n","> - **Masked Language Modeling**:\n","    - The input ids are passed through the TabRowEmbeddings layer to obtain the embeddings of the tabular data.\n","    - The embeddings are then passed to the BertModel.\n","    - The output of the BertModel is passed through the MLM layers to obtain the predictions at field level.\n","    - The predictions are compared to the masked LM labels at field level and the cross entropy loss is computed.\n","> - **Classification**:  \n","    - The input ids are passed through the TabRowEmbeddings layer to obtain the embeddings of the tabular data.\n","    - The embeddings are then passed to the BertModel.\n","    - The CLS embedding is extracted from the output of the BertModel and it is passed through the classification layers to obtain a prediction for each sample.\n","    - The predictions are compared to the targets (1 for Fraud, 0 for No Fraud) and the Binary Cross Entropy Loss is computed.  \n","    \n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MpSnLdxlYNVz"},"outputs":[],"source":["class TabRowEmbeddings(nn.Module):\n","    \"\"\"\n","    Custom embedding class for tabular row data.\n","\n","    This custom class is designed handle embeddings for tabular data where each row consists of multiple columns.\n","    Each individual token is mapped to an embedding.\n","    The sequence is then passed to a transformer encoder to capture relationships between columns.\n","    A final linear projection transform the embeddings to the desired hidden size.\n","    \"\"\"\n","    def __init__(self,\n","                 config: CustomBertConfig) -> None:\n","        \"\"\"\n","        Initializes the TabRowEmbeddings class.\n","\n","        Args:\n","        - config (CustomBertConfig):\n","            CustomBertConfig object with attributes:\n","            - vocab_size: Vocabulary size of the model.\n","            - field_hidden_size: Hidden size for field embeddings.\n","            - ncols: The number of columns in the tabular data.\n","            - hidden_size: Hidden size for the output row embeddings.\n","            - pad_token_id (optional): Index used for padding.\n","        \"\"\"\n","        super().__init__()\n","        self.word_embeddings = nn.Embedding(num_embeddings=config.vocab_size,\n","                                            embedding_dim=config.field_hidden_size,\n","                                            padding_idx=getattr(config, 'pad_token_id', 0))\n","        encoder_layer = nn.TransformerEncoderLayer(d_model=config.field_hidden_size,\n","                                                   nhead=8,\n","                                                   dim_feedforward=config.field_hidden_size,\n","                                                   batch_first=True)\n","        self.transformer_encoder = nn.TransformerEncoder(encoder_layer=encoder_layer,\n","                                                         num_layers=1)\n","        self.linear = nn.Linear(in_features=config.field_hidden_size*config.ncols,\n","                                out_features=config.hidden_size)\n","        self._init_model_weights()\n","\n","    def forward(self,\n","                input_ids: torch.Tensor) -> torch.Tensor:\n","        \"\"\"\n","        Forward step of TabRowEmbeddings.\n","\n","        Args:\n","            - input_ids:\n","                Tensor of shape [batch_size, seq_len+1, ncols+1] containing the token ids.\n","\n","        Returns:\n","            - input_embeds:\n","                Tensor of shape [batch_size, seq_len+1, hidden_size] containing the output row embeddings.\n","        \"\"\"\n","\n","        inputs_embeds = self.word_embeddings(input_ids) #[batch_size, seq_len+1, ncols+1, field_hidden_size]\n","        embeds_shape = inputs_embeds.shape\n","        # reshape the embeddings\n","        inputs_embeds = inputs_embeds.view(embeds_shape[0]*embeds_shape[1], embeds_shape[2], -1)  #[batch_size*(seq_len+1), ncols+1, field_hidden_size]\n","        # passing through the transformer encoder\n","        inputs_embeds = self.transformer_encoder(inputs_embeds)\n","        # reshape the embeddings to have a single row embedding\n","        inputs_embeds = inputs_embeds.contiguous().view(embeds_shape[0], embeds_shape[1], -1)  # [batch_size, seq_len+1, (ncols+1)*field_hidden_size]\n","        # final linear projection to hidden size\n","        inputs_embeds = self.linear(inputs_embeds) # [batch_size, seq_len+1, hidden_size]\n","        return inputs_embeds\n","\n","    def _init_model_weights(self):\n","        \"\"\"\n","        Initializes the weights of the model.\n","        \"\"\"\n","        for module in self.modules():\n","            if isinstance(module, nn.Linear):\n","                nn.init.xavier_uniform_(module.weight)\n","                if module.bias is not None:\n","                    module.bias.data.zero_()\n","            elif isinstance(module, nn.Embedding):\n","                nn.init.uniform_(module.weight, -1.0, 1.0)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VHsS7T3zYNV0"},"outputs":[],"source":["class HierarchicalBertLM(PreTrainedModel):\n","    def __init__(self,\n","                 config: BertConfig,\n","                 vocab: Vocab,\n","                 mode: str='mlm') -> None:\n","        \"\"\"\n","        Initializes the HierarchicalBertLM class. It can be used for masked LM and classification tasks.\n","\n","        Args:\n","            - config (CustomBertConfig):\n","                CustomBertConfig object with attributes:\n","                - vocab_size: Vocabulary size of the model.\n","                - field_hidden_size: Hidden size for field embeddings.\n","                - ncols: The number of columns in the tabular data.\n","                - hidden_size: Hidden size for the output row embeddings.\n","                - pad_token_id (optional): Index used for padding.\n","                - hidden_act (optional): Activation function used in the feedforward layer.\n","                - layer_norm_eps (optional): Epsilon value for layer normalization.\n","            - vocab (Vocab):\n","                Vocab object containing the vocabulary of the model.\n","            - mode (str, 'mlm' or 'classification'):\n","                Mode of the model. If 'mlm', the model is trained with masked LM. If 'classification', the model is trained for classification.\n","        \"\"\"\n","        super().__init__(config)\n","        self.config = config\n","        self.vocab = vocab\n","        self.mode = mode\n","        # tabular embeddings\n","        self.tabular_row_embeddings = TabRowEmbeddings(self.config)\n","        # bert model for sequence of rows\n","        self.bert = BertModel(config)\n","        # MLM-specific layers\n","        self.mlm_linear = nn.Linear(in_features=config.field_hidden_size,\n","                                    out_features=config.hidden_size)\n","        self.layer_norm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n","        self.decoder = nn.Linear(config.hidden_size, config.vocab_size, bias=False)\n","        self.bias = nn.Parameter(torch.zeros(config.vocab_size))\n","        self.decoder.bias = self.bias\n","        if isinstance(config.hidden_act, str):\n","            self.activation_function = ACT2FN[config.hidden_act]\n","        else:\n","            self.activation_function = config.hidden_act\n","        # CrossEntropyLoss for masked LM\n","        self.loss_fct_mlm = nn.CrossEntropyLoss()\n","        # precompute the global ids for each column\n","        if self.mode=='mlm':\n","            self.precomputed_global_ids = {key: self.vocab.get_global_ids(key) for key in self.vocab.cols_for_vocab}\n","        # classification-specific layers\n","        self.hidden_layer_cls = nn.Linear(config.hidden_size, 256)\n","        self.dropout = nn.Dropout(0.1)\n","        self.output_layer_cls = nn.Linear(256, 1)\n","        self.loss_fct_cls = nn.BCELoss()\n","        self._init_model_weights()\n","\n","    def compute_masked_lm_loss(self,\n","                               sequence_output: torch.Tensor,\n","                               masked_lm_labels: torch.Tensor,\n","                               outputs: tuple) -> torch.Tensor:\n","        \"\"\"\n","        Computes the masked LM loss.\n","\n","        Args:\n","            - sequence_output (torch.Tensor):\n","                Tensor of shape [batch_size, seq_len, hidden_size] containing the output of the BERT model.\n","            - masked_lm_labels (torch.Tensor):\n","                Tensor of shape [batch_size, seq_len, ncols] containing the masked LM labels.\n","            - outputs (torch.Tensor):\n","                Tuple containing the outputs of the BERT model.\n","\n","        Returns:\n","            - total_masked_lm_loss (torch.Tensor):\n","                Tensor containing the total masked LM loss\n","        \"\"\"\n","        # we must reshape the output to reconstruct field embeddings\n","        output_shape = sequence_output.shape # [batch_size, seq_len, hidden_size]\n","        expected_shape = [output_shape[0], output_shape[1]*self.config.ncols, -1] # [batch_size, seq_len*ncols, field_hidden_size]\n","        sequence_output = sequence_output.view(expected_shape)\n","        masked_lm_labels = masked_lm_labels.view(expected_shape[0], -1) # [batch_size, seq_len*ncols]\n","        # pass the output of BERT through the feedforward layer, output shape [batch_size, seq_len*ncols, hidden_size]\n","        hidden_state = self.mlm_linear(sequence_output)\n","        hidden_state = self.activation_function(hidden_state)\n","        hidden_state = self.layer_norm(hidden_state)\n","        prediction_scores = self.decoder(hidden_state) # [batch_size, seq_len*ncols, vocab_size]\n","        outputs = (prediction_scores, ) + outputs[2:]\n","        total_masked_lm_loss = 0\n","        seq_len = prediction_scores.size(1)\n","        # get the field names\n","        field_names = self.vocab.cols_for_vocab\n","        # iterate over the field names\n","        for index, key in enumerate(field_names):\n","            # get the global ids for the field\n","            col_ids = list(range(index, seq_len, len(field_names)+1))\n","            global_ids_field = self.precomputed_global_ids[key]\n","            # remember that prediction_scores has shape [batch_size, seq_len*ncols, vocab_size], so we need to select the right columns.\n","            # we select the prediction scores for the particular field and from them we select the scores only corresponding to the global ids of the field\n","            prediction_scores_field = prediction_scores[:, col_ids, :][:, :, global_ids_field]  # [batch_size, seq_len, K] where K is the number of unique tokens in the field (the vocab size of the field)\n","            # selection of the masked LM labels for the field\n","            masked_lm_labels_field = masked_lm_labels[:, col_ids]\n","            # map the global ids to local ids\n","            masked_lm_labels_field_local = self.vocab.map_global_to_local(global_ids=masked_lm_labels_field)\n","            # compute the masked LM loss for the field\n","            masked_lm_loss_field = self.loss_fct_mlm(prediction_scores_field.view(-1, len(global_ids_field)),\n","                                                     masked_lm_labels_field_local.view(-1))\n","            if not torch.isnan(masked_lm_loss_field):\n","                total_masked_lm_loss += masked_lm_loss_field\n","        return total_masked_lm_loss\n","\n","    def forward(self,\n","                input_ids=None,\n","                attention_mask=None,\n","                labels=None) -> dict:\n","        \"\"\"\n","        Forward step of HierarchicalBertLM. Works for both masked LM and classification.\n","\n","        Args:\n","            - input_ids (torch.Tensor):\n","                Tensor of shape [batch_size, seq_len+1, ncols+1] containing the token ids.\n","            - attention_mask (torch.Tensor):\n","                Tensor of shape [batch_size, seq_len+1, ncols+1] containing the attention mask.\n","            - labels (torch.Tensor):\n","                Tensor of shape [batch_size, seq_len+1, 1] or [batch_size, 1] containing the masked LM labels or the classification targets.\n","\n","        Returns:\n","            - dict:\n","                Dictionary containing the loss and the predictions (if present).\n","        \"\"\"\n","        # construct the embeddings of the tabular data, output shape [batch_size, seq_len, hidden_size]\n","        inputs_embeds = self.tabular_row_embeddings(input_ids)\n","        # pass the time series of rows through BERT\n","        outputs = self.bert(inputs_embeds=inputs_embeds)\n","        sequence_output = outputs[0]\n","        if self.mode=='mlm':\n","            total_loss = self.compute_masked_lm_loss(sequence_output,\n","                                                     labels,\n","                                                     outputs)\n","            return {'loss': total_loss}\n","        # classification task\n","        elif self.mode=='classification':\n","            # extract the CLS token\n","            cls_embedding = sequence_output[:, 0, :]\n","            # pass the output of BERT through the hidden layer\n","            x = self.hidden_layer_cls(cls_embedding)\n","            x = self.activation_function(x)\n","            x = self.dropout(x)\n","            logits = self.output_layer_cls(x)\n","            probs = torch.sigmoid(logits)\n","            # reshape the probs and the labels\n","            probs_flat = probs.view(-1)\n","            labels_flat = labels.view(-1)\n","            total_loss = self.loss_fct_cls(probs_flat, labels_flat)\n","            return {'loss': total_loss,\n","                    'predictions': probs_flat,\n","                    'labels': labels_flat}\n","        else:\n","            raise ValueError(\"Neither masked_lm_labels nor labels are provided for the forward pass.\")\n","\n","    def freeze_model_except_nlayers(self,\n","                                    n: int=2) -> None:\n","        \"\"\"\n","        Freezes all the layers of the encoder model except the last n layers.\n","\n","        Args:\n","            - n (int):\n","                Number of layers to keep unfrozen. Default to 2.\n","        \"\"\"\n","        # freeze all the layers\n","        for param in self.tabular_row_embeddings.parameters():\n","            param.requires_grad = False\n","        for param in self.bert.parameters():\n","            param.requires_grad = False\n","        # unfreeze the last n layers\n","        for param in self.bert.encoder.layer[-n:].parameters():\n","            param.requires_grad = True\n","        # unfreeze the classification layers (if present)\n","        if self.mode=='classification':\n","            for param in self.hidden_layer_cls.parameters():\n","                param.requires_grad = True\n","            for param in self.output_layer_cls.parameters():\n","                param.requires_grad = True\n","\n","    def _init_model_weights(self) -> None:\n","        \"\"\"Initializes the weights of the model.\"\"\"\n","        # initialize weights for MLM layers\n","        if self.mode == 'mlm':\n","            init.xavier_uniform_(self.mlm_linear.weight)\n","            self.mlm_linear.bias.data.zero_()\n","            init.xavier_uniform_(self.decoder.weight)\n","            self.decoder.bias.data.zero_()\n","        # initialize weights for classification layers\n","        if self.mode == 'classification':\n","            init.xavier_uniform_(self.hidden_layer_cls.weight)\n","            self.hidden_layer_cls.bias.data.zero_()\n","            init.xavier_uniform_(self.output_layer_cls.weight)\n","            self.output_layer_cls.bias.data.zero_()"]},{"cell_type":"markdown","metadata":{"id":"END69O9fXr6a"},"source":["> **_Important_**:\n",">\n","> **The following code is not meant to be executed. It is only used to show how to instantiate the model.\n","> The models will be instantiated in the training manager based on the mode (mlm or classification).**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"InpLyfQ-S_k6"},"outputs":[],"source":["## NO NEED TO RUN THIS CELL\n","model_mlm = HierarchicalBertLM(config=config,\n","                               vocab=vocab,\n","                               mode='mlm')\n","\n","model_classification = HierarchicalBertLM(config=config,\n","                                          vocab=vocab,\n","                                          mode='classification')"]},{"cell_type":"markdown","metadata":{"id":"aORbbCAAgra9"},"source":["---\n","# **TRAINING AND EVALUATION**"]},{"cell_type":"markdown","metadata":{"id":"N1aqqgJpNj0b"},"source":["> ## **Weights & Biases**\n",">\n","> In order to log the training process and the metrics, we will use [Weights & Biases](https://wandb.ai/site).\n",">\n","> **_Important_:**\n",">\n","> **You can create a free account and login from the notebook running the following cell.**\n",">\n","> **While running the cell, you will be prompted to enter your API key. You can find your API key [here](https://wandb.ai/authorize).**\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":69},"executionInfo":{"elapsed":6964,"status":"ok","timestamp":1705233234464,"user":{"displayName":"domenico meconi","userId":"12240534909735751246"},"user_tz":-60},"id":"qbuRUGWWsxfF","outputId":"520d6b1e-43b1-4489-95ed-67e644715365"},"outputs":[{"data":{"application/javascript":"\n        window._wandbApiKey = new Promise((resolve, reject) => {\n            function loadScript(url) {\n            return new Promise(function(resolve, reject) {\n                let newScript = document.createElement(\"script\");\n                newScript.onerror = reject;\n                newScript.onload = resolve;\n                document.body.appendChild(newScript);\n                newScript.src = url;\n            });\n            }\n            loadScript(\"https://cdn.jsdelivr.net/npm/postmate/build/postmate.min.js\").then(() => {\n            const iframe = document.createElement('iframe')\n            iframe.style.cssText = \"width:0;height:0;border:none\"\n            document.body.appendChild(iframe)\n            const handshake = new Postmate({\n                container: iframe,\n                url: 'https://wandb.ai/authorize'\n            });\n            const timeout = setTimeout(() => reject(\"Couldn't auto authenticate\"), 5000)\n            handshake.then(function(child) {\n                child.on('authorize', data => {\n                    clearTimeout(timeout)\n                    resolve(data)\n                });\n            });\n            })\n        });\n    ","text/plain":["<IPython.core.display.Javascript object>"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"]},{"data":{"text/plain":["True"]},"execution_count":20,"metadata":{},"output_type":"execute_result"}],"source":["## RUN THIS CELL - NO CHANGES NEEDED\n","import wandb\n","\n","wandb.login()"]},{"cell_type":"markdown","metadata":{"id":"WaDwLWPoPow7"},"source":["> ## **MLM Training Configuration**\n",">\n","> In this cell we define a dictionary containing the training parameters to train the Masked Language Model.\n",">\n","> The parameters are passed to the TrainingArguments class from the transformers library, that is used to instantiate the Trainer class. Thus, be sure that the parameters are valid for the TrainingArguments class. A check is performed in the TrainingManager class but it is better to check them before.\n",">\n","> **_Important_:**\n",">\n","> **Note if you're not planning to train the model, you can skip this cell. Pretrained models will be loaded in the next sections.**\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qcfZQ4RITOEY"},"outputs":[],"source":["## NO NEED TO RUN THIS CELL UNLESS YOU WANT TO TRAIN THE MODEL\n","training_mlm_config_dict = {\n","    'per_device_train_batch_size': 128,\n","    'per_device_eval_batch_size': 128,\n","    'num_train_epochs': 50,\n","    'logging_strategy': 'steps',\n","    'logging_first_step': True,\n","    'logging_steps': 1,\n","    'save_strategy': 'steps',\n","    'save_steps': 750,\n","    'evaluation_strategy': 'steps',\n","    'eval_steps': 250,\n","    'load_best_model_at_end': True,\n","    'disable_tqdm': False,\n","    'seed': 2024,\n","    'learning_rate': 5e-5,\n","    'report_to':'wandb',\n","    'lr_scheduler_type':'constant'}"]},{"cell_type":"markdown","metadata":{"id":"Nn4xhisRRwwN"},"source":["> ## **Training Manager Class**\n",">\n","> The TrainingManager class is responsible for setting up the model, the data collator, the training arguments, and the HuggingFace Trainer.\n",">\n","> - **Class Initialization:**\n",">     - To initialize the class, we need to provide the model configuration dictionary, the training configuration dictionary, the training, validation, and test sets, the root directory, the project name, the model name, the mode (either 'mlm' or 'classification'), and the path to the pretrained model checkpoint (only required for 'classification' mode).\n",">     - The model configuration dictionary contains the model parameters to be logged.\n",">     - The training configuration dictionary contains the training parameters for the TrainingArguments class from HuggingFace. Be sure to check the [documentation](https://huggingface.co/transformers/main_classes/trainer.html#trainingarguments) for the list of available parameters. A check is performed to ensure that only valid parameters are provided.\n",">     - The training, validation, and test sets are instances of the PRSADataset class.\n",">     - The root directory is the directory where the output and logs directories will be created.\n",">     - The project name is the name of the project for logging on wandb.\n",">     - The model name is the name of the model for logging on wandb.\n",">     - The mode is either 'mlm' or 'classification'.\n",">     - The pretrained model path is the path to the pretrained model checkpoint.\n",">\n","> - **Directories and Logging:**\n",">     - The setup_directories() method sets up the checkpoints and logs directories.\n",">     - The setup_wandb() method sets up wandb for logging.\n",">\n","> - **Tokenizer and Collator:**\n",">     - The setup_tokenizer() method sets up the tokenizer needed in the data collator.\n",">     - The setup_collator() method sets up the data collator for training. If the mode is 'mlm', the data collator returns the labels for masked language modeling. If the mode is 'classification', the data collator returns the labels for classification.\n",">\n","> - **Model:**\n",">     - The setup_model() method sets up the model for training. If a pretrained model path is provided, the model is initialized from the pretrained checkpoint. If the mode is 'mlm', the model is trained with masked language modeling. If the mode is 'classification', the model is initialized from the pretrained checkpoint (after MLM training) and trained for classification. The model is frozen except for the last n (default 3) layers.\n",">\n","> - **Training:**\n",">     - The setup_training() method sets up the training arguments and trainer.\n",">     - The train() method must be called to train the model.\n",">     - The evaluate() method can be used to evaluate the model on the validation or test set."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2xSKHqZsSz0-"},"outputs":[],"source":["class TrainingManager:\n","    def __init__(self,\n","                 model_config_dict: dict,\n","                 training_config_dict: dict,\n","                 vocab: Vocab,\n","                 train_set: TransactionDataset,\n","                 val_set: TransactionDataset,\n","                 test_set: TransactionDataset,\n","                 root_dir: str='/content',\n","                 project_name: str='CreditTabBert',\n","                 model_name: str='credit-0',\n","                 mode: str='mlm',\n","                 pretrained_model_path: str=None,\n","                 layers_to_unfreeze: Optional[int] = 3) -> None:\n","        \"\"\"\n","        Initializes the TrainingManager class.\n","\n","        Args:\n","            - model_config_dict (dict): configuration dictionary containing model and training parameters to be logged.\n","            - training_config_dict (dict): configuration dictionary containing training parameters for training_args\n","            - vocab (Vocab): Vocab object containing the vocabulary of the model.\n","            - data_collator (CustomDataCollator): CustomDataCollator object.\n","            - train_set (TransactionDataset): TransactionDataset object containing the training data.\n","            - val_set (TransactionDataset): TransactionDataset object containing the validation data.\n","            - test_set (TransactionDataset): TransactionDataset object containing the test data.\n","            - root_dir (str): Root directory of the project. Defaults to '/content'.\n","            - project_name (str): Name of the project. Defaults to 'CreditTabBert'.\n","            - model_name (str): Name of the model. Defaults to 'credit-0'.\n","            - mode (str): Mode of the model. Either 'mlm' or 'classification'.\n","            - pretrained_model_path (str): Path to the pretrained model checkpoint.\n","            - layers_to_unfreeze (int): numbre of layers of TabBert to unfreeze.\n","        \"\"\"\n","        self.model_config_dict = model_config_dict\n","        self.model_config =  CustomBertConfig(**self.model_config_dict)\n","        self.training_config_dict = training_config_dict\n","        self.vocab = vocab\n","        self.train_set = train_set\n","        self.val_set = val_set\n","        self.test_set = test_set\n","        self.root_dir = root_dir\n","        self.project_name = project_name\n","        self.model_name = model_name\n","        self.mode = mode\n","        self.pretrained_model_path = pretrained_model_path\n","        self.layers_to_unfreeze = layers_to_unfreeze\n","        self._validate_attributes()\n","        self.setup_directories()\n","        self.setup_model()\n","        self.setup_wandb()\n","        self.setup_tokenizer()\n","        self.setup_collator()\n","        self.setup_training()\n","\n","    def _validate_attributes(self) -> None:\n","        \"\"\"Helper function to validate the attributes.\"\"\"\n","        validations = [\n","            (self.model_config_dict, dict, '\"model_config_dict\" must be a dictionary'),\n","            (self.training_config_dict, dict, '\"training_config_dict\" must be a dictionary'),\n","            (self.vocab, Vocab, '\"vocab\" must be an instance of Vocab'),\n","            (self.train_set, TransactionDataset, '\"train_set\" must be an instance of TransactionDataset'),\n","            (self.val_set, TransactionDataset, '\"val_set\" must be an instance of TransactionDataset'),\n","            (self.test_set, TransactionDataset, '\"test_set\" must be an instance of TransactionDataset'),\n","            (self.root_dir, (str, type(None)), '\"root_dir\" must be a string or None'),\n","            (self.project_name, str, '\"project_name\" must be a string'),\n","            (self.model_name, str, '\"model_name\" must be a string'),\n","            (self.mode, str, '\"mode\" must be a string'),\n","            (self.pretrained_model_path, (str, type(None)), '\"pretrained_model_path\" must be a string or None'),\n","            (self.layers_to_unfreeze, int, '\"layers_to_unfreeze\" must be an integer')\n","        ]\n","        for var, var_type, err_msg in validations:\n","            if not isinstance(var, var_type):\n","                raise TypeError(f'{err_msg}. Got {type(var)} instead.')\n","        if self.mode not in ['mlm', 'classification']:\n","            raise ValueError('\"mode\" must be either \"mlm\" or \"classification\"')\n","        if self.mode == 'classification' and self.pretrained_model_path is None:\n","            raise ValueError('\"pretrained_model_path\" is required for \"classification\" mode')\n","        if self.mode == 'classification' and not os.path.exists(self.pretrained_model_path):\n","            raise ValueError(f'\"{self.pretrained_model_path}\" does not exist')\n","\n","        valid_params  = set(inspect.signature(TrainingArguments).parameters.keys())\n","        input_params = set(self.training_config_dict.keys())\n","        invalid_params = input_params - valid_params\n","        if invalid_params:\n","            raise ValueError(f'Invalid parameters in training_config_dict: {invalid_params}')\n","\n","    def setup_directories(self) -> None:\n","        \"\"\"Sets up the output and logs directories.\"\"\"\n","        try:\n","            base_dir = Path(self.root_dir) / f'output' / self.mode\n","            base_dir.mkdir(parents=True, exist_ok=True)\n","            self.CHECKPOINT_DIR = base_dir / 'checkpoints' / self.model_name\n","            self.CHECKPOINT_DIR.mkdir(parents=True, exist_ok=True)\n","            self.LOGS_DIR = base_dir / 'logs' / self.model_name\n","            self.LOGS_DIR.mkdir(parents=True, exist_ok=True)\n","        except Exception as e:\n","            logger.error(f\"An error occurred while setting up directories: {e}\")\n","            raise\n","\n","    def setup_collator(self) -> None:\n","        \"\"\"Sets up the data collator for training.\"\"\"\n","        try:\n","            self.data_collator = CustomDataCollator(tokenizer=self.tokenizer,\n","                                                    mlm_probability=self.model_config.mlm_probability,\n","                                                    mlm=self.mode=='mlm')\n","        except Exception as e:\n","            logger.error(f\"An error occurred while setting up the data collator: {e}\")\n","            raise\n","\n","    def setup_tokenizer(self) -> None:\n","        \"\"\"Sets up the tokenizer for training.\"\"\"\n","        try:\n","            self.tokenizer = BertTokenizerFast(vocab_file=self.vocab.vocab_file_for_bert,\n","                                               do_lower_case=False,\n","                                               **self.vocab.get_special_tokens())\n","        except Exception as e:\n","            logger.error(f\"An error occurred while setting up the tokenizer: {e}\")\n","            raise\n","\n","    def setup_model(self) -> None:\n","        \"\"\"\n","        Sets up the model for training.\n","        If the mode is 'mlm', the model is initialized from scratch and trained with masked language modeling.\n","        If a pretrained_model_path is provided, the model is initialized from the pretrained checkpoint.\n","        If the mode is 'classification', the model is initialized from a pretrained checkpoint and trained for classification.\n","        \"\"\"\n","        try:\n","            if self.pretrained_model_path:\n","                self.model = HierarchicalBertLM.from_pretrained(self.pretrained_model_path,\n","                                                                config=self.model_config,\n","                                                                vocab=self.vocab,\n","                                                                mode=self.mode,\n","                                                                ignore_mismatched_sizes=True)\n","            else:\n","                self.model = HierarchicalBertLM(config=self.model_config,\n","                                                vocab=self.vocab,\n","                                                mode=self.mode)\n","            if self.mode == 'classification':\n","                self.model.freeze_model_except_nlayers(n=self.layers_to_unfreeze)\n","        except Exception as e:\n","            logger.error(f\"An error occurred while setting up the model: {e}\")\n","            raise\n","\n","    def setup_wandb(self) -> None:\n","        \"\"\"Sets up wandb for logging.\"\"\"\n","        try:\n","            wandb.init(config=self.model_config_dict,\n","                       project=self.project_name,\n","                       name=self.model_name,\n","                       group=self.mode,\n","                       dir=str(self.LOGS_DIR))\n","            wandb.config.update(self.model_config_dict)\n","        except Exception as e:\n","            logger.error(f\"An error occurred while setting up wandb: {e}\")\n","            raise\n","\n","    def setup_training(self) -> None:\n","        \"\"\"Sets up the training arguments and trainer.\"\"\"\n","        try:\n","            self.training_args = TrainingArguments(output_dir=str(self.CHECKPOINT_DIR),\n","                                                   logging_dir=str(self.LOGS_DIR),\n","                                                   **self.training_config_dict)\n","            if self.mode == 'classification':\n","                self.trainer = Trainer(model=self.model,\n","                                    args=self.training_args,\n","                                    data_collator=self.data_collator,\n","                                    train_dataset=self.train_set,\n","                                    eval_dataset=self.val_set,\n","                                    compute_metrics = self.compute_metrics)\n","            else:\n","                self.trainer = Trainer(model=self.model,\n","                                args=self.training_args,\n","                                data_collator=self.data_collator,\n","                                train_dataset=self.train_set,\n","                                eval_dataset=self.val_set)\n","\n","        except Exception as e:\n","            logger.error(f\"An error occurred while setting up training: {e}\")\n","            raise\n","\n","    def _cleanup(self) -> None:\n","        \"\"\"Helper function to terminate wandb process.\"\"\"\n","        if wandb.run:\n","            wandb.finish()\n","        logger.info('Cleanup completed.')\n","\n","    def train(self,\n","              resume_from_checkpoint: Union[bool, str]=None) -> None:\n","        \"\"\"\n","        Trains the model.\n","\n","        Args:\n","            - resume_from_checkpoint (Union[bool, str], optional):\n","                If a str, local path to a saved checkpoint as saved by a previous instance of Trainer.\n","                If a bool and equals True, load the last checkpoint in args.output_dir as saved by a previous instance of Trainer.\n","                If None, training starts from scratch.\n","        \"\"\"\n","        if not wandb.run:\n","            self.setup_wandb()\n","        try:\n","            self.trainer.train(resume_from_checkpoint=resume_from_checkpoint)\n","        except Exception as e:\n","            logger.error(f\"An error occurred during training: {e}\")\n","        finally:\n","            self._cleanup()\n","\n","    def compute_metrics(self, out):\n","        predictions = out.predictions[0]\n","        labels = out.predictions[1]\n","        threshold = 0.5\n","        binary_predictions = (predictions > threshold).astype(int)\n","        precision = precision_score(labels, binary_predictions)\n","        recall = recall_score(labels, binary_predictions)\n","        f1 = f1_score(labels, binary_predictions)\n","        accuracy = accuracy_score(labels, binary_predictions)\n","        return {'precision': precision,\n","                'recall': recall,\n","                'f1': f1,\n","                'accuracy': accuracy}\n","\n","    def evaluate(self,\n","                 test=False):\n","        \"\"\"\n","        Evaluates the model on the validation or test set based on the value of test.\n","\n","        Args:\n","            - test (bool): If True, evaluate on the test set. Else, evaluate on the validation set.\n","        \"\"\"\n","        if self.mode == 'classification':\n","            if test:\n","                if self.test_set is None:\n","                    raise ValueError('Test set is None. Cannot evaluate.')\n","                else:\n","                    predictions = self.trainer.predict(self.test_set)\n","                    metrics = self.compute_metrics(predictions)\n","                    binary_predictions = (predictions.predictions[0] > 0.5).astype(int)\n","                    return metrics, predictions.predictions[0], predictions.predictions[1]\n","        out = self.trainer.evaluate()\n","        return out\n"]},{"cell_type":"markdown","metadata":{"id":"Pg28JkAYCfiz"},"source":["\n","> ## **MLM Training**\n",">\n","> **_Important_:**\n",">\n","> **Run this cell if you want to initialize the TrainingManager class for MLM training and run the train() method.**\n",">\n","> **A pretrained model checkpoint is provided to evaluate the model on the val/test set. If you prefer to train the model from scratch, set the pretrained_model_path to None.**\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9Y9X2NJ0Vr8g"},"outputs":[],"source":["## RUN THIS CELL\n","## SET TO NONE TO TRAIN THE MODEL FROM SCRATCH\n","mlm_pretrained_model_path =  os.path.join(ROOT_DIR, 'output/mlm/checkpoints/card-model/checkpoint-final')\n","\n","training_manager_mlm = TrainingManager(model_config_dict=model_config_values,\n","                                       training_config_dict=training_mlm_config_dict,\n","                                       vocab=vocab,\n","                                       train_set=train_dataset,\n","                                       val_set=val_dataset,\n","                                       test_set=test_dataset,\n","                                       root_dir=ROOT_DIR,\n","                                       model_name='card-model',\n","                                       mode='mlm',\n","                                       pretrained_model_path=mlm_pretrained_model_path)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wpGX9lj8V8uF"},"outputs":[],"source":["## NO NEED TO RUN THIS CELL\n","training_manager_mlm.train()"]},{"cell_type":"markdown","metadata":{"id":"ClUN8QLOx2nw"},"source":["> ## **MLM Evaluation**\n",">\n","> The following cell evaluates the model on the validation set.\n","> It returns the Cross Entropy Loss for MLM.\n",">\n","> **_Important_:**\n",">\n","> **If you run the .train() cell without finishing the training process, make sure to run again the training manager cell with the provided checkpoint.**"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":144},"executionInfo":{"elapsed":771602,"status":"ok","timestamp":1705234049506,"user":{"displayName":"domenico meconi","userId":"12240534909735751246"},"user_tz":-60},"id":"goNKrYHXAR3G","outputId":"ed2a629d-44a3-40e7-dba3-5490ed52c585"},"outputs":[{"name":"stderr","output_type":"stream","text":["You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"]},{"data":{"text/html":["\n","    <div>\n","      \n","      <progress value='778' max='778' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [778/778 12:49]\n","    </div>\n","    "],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/plain":["{'eval_loss': 11.246283531188965,\n"," 'eval_runtime': 771.5818,\n"," 'eval_samples_per_second': 128.978,\n"," 'eval_steps_per_second': 1.008}"]},"execution_count":24,"metadata":{},"output_type":"execute_result"}],"source":["## RUN THIS CELL\n","training_manager_mlm.trainer.evaluate()"]},{"cell_type":"markdown","metadata":{"id":"2LsFQxvAgFS6"},"source":["> ## **Classification Training Configuration**\n",">\n","> In this cell we define a dictionary containing the training parameters to train the model for the classification task.\n",">\n","> The parameters are passed to the TrainingArguments class from the transformers library, that is used to instantiate the Trainer class. Thus, be sure that the parameters are valid for the TrainingArguments class. A check is performed in the TrainingManager class but it is better to check them before.\n",">\n","> **_Important_:**\n",">\n","> **Run this cell as it is.**\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0yarVJSoz1Yz"},"outputs":[],"source":["training_cls_config_dict = {\n","    'per_device_train_batch_size': 128,\n","    'per_device_eval_batch_size': 128,\n","    'num_train_epochs': 10,\n","    'logging_strategy': 'steps',\n","    'logging_first_step': True,\n","    'logging_steps': 1,\n","    'save_strategy': 'steps',\n","    'save_steps': 100,\n","    'evaluation_strategy': 'steps',\n","    'eval_steps': 100,\n","    'load_best_model_at_end': True,\n","    'disable_tqdm': False,\n","    'seed': 2024,\n","    'learning_rate': 5e-5,\n","    'report_to':'wandb'}"]},{"cell_type":"markdown","metadata":{"id":"V4K462mYC11n"},"source":["\n","> ## **Classification Training**\n",">\n","> **_Important_:**\n",">\n","> **Run this cell if you want to initialize the TrainingManager class for classification training and run the train() method.**\n",">\n","> **A pretrained model checkpoint is provided to evaluate the model on the val/test set. If you prefer to train the model from scratch (starting from pretrained mlm), set the pretrained_model_path to mlm_pretrained_model_path.**\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VA6MJCcUXiDw"},"outputs":[],"source":["## RUN THIS CELL TO SET UP THE TRAINING MANAGER FOR CLASSIFICATION - NO CHANGES NEEDED\n","\n","## SET TO 'mlm_pretrained_model_path' TO TRAIN THE MODEL FROM THE PRETRAINED BERT (from mlm)\n","cls_pretrained_model_path = os.path.join(ROOT_DIR, 'output/classification/checkpoints/card-model-cls/checkpoint-final')\n","\n","training_manager_cls = TrainingManager(model_config_dict=model_config_values,\n","                                       training_config_dict=training_cls_config_dict,\n","                                       vocab=vocab,\n","                                       train_set=train_dataset_cls,\n","                                       val_set=val_dataset,\n","                                       test_set=test_dataset,\n","                                       root_dir=ROOT_DIR,\n","                                       model_name='card-model-cls',\n","                                       mode='classification',\n","                                       pretrained_model_path=cls_pretrained_model_path)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hA7BQvWWYNeV"},"outputs":[],"source":["## NO NEEED TO RUN THIS CELL\n","training_manager_cls.train()"]},{"cell_type":"markdown","metadata":{"id":"UEK4L4MfDOGx"},"source":["> ## **Classification Evaluation**\n",">\n","> The following cell evaluates the model on the validation/test set based on the value of test.\n","> The method evaluate returns the metrics, the predictions and the labels.\n",">\n","> The following metrics are computed:\n",">\n","> - **F1**: A metric that combines precision and recall, providing a balanced measure of a model's performance by considering both false positives and false negatives.\n","> - **Precision**: It measures the accuracy of positive predictions, representing the ratio of true positives to the total predicted positives.\n","> - **Recall**: It measures the ability of the model to capture all relevant instances of the positive class. It is the ratio of true positives to the total actual positives.\n","> - **Accuracy**: It represents the overall correctness of the model's predictions, calculated as the ratio of correct predictions to the total number of instances.\n",">\n","> In our evaluation, the model achieved an F1 score of 0.52, which is notably lower than the 0.76 F1 score reported in the reference paper. This discrepancy in performance can be primarily attributed to the difference in the volume of training data used. Our model was trained on a dataset comprising 5 million samples, a decision driven by constraints in computational resources and time. In contrast, the paper's model was trained on the full dataset, which encompasses 24 million samples.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":176},"executionInfo":{"elapsed":147366,"status":"ok","timestamp":1705234257617,"user":{"displayName":"domenico meconi","userId":"12240534909735751246"},"user_tz":-60},"id":"Q1tvFM4MQHri","outputId":"725f1a80-d51b-4ff7-c3cc-fd560bc75d4a"},"outputs":[{"name":"stderr","output_type":"stream","text":["You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"]},{"data":{"text/html":[],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/plain":["({'precision': 0.6470588235294118,\n","  'recall': 0.4408014571948998,\n","  'f1': 0.5243770314192849,\n","  'accuracy': 0.9955810114350137},\n"," array([0.00013916, 0.00012273, 0.00011576, ..., 0.00011405, 0.00010707,\n","        0.00012873], dtype=float32),\n"," array([0., 0., 0., ..., 0., 0., 0.], dtype=float32))"]},"execution_count":27,"metadata":{},"output_type":"execute_result"}],"source":["## RUN THIS CELL\n","## SET TEST TO FALSE/TRUE TO EVALUATE THE MODEL ON THE VAL/TEST SET\n","training_manager_cls.evaluate(test=True)"]},{"cell_type":"markdown","metadata":{"id":"7TcN561MojKk"},"source":["---\n","# **WANDB**\n","\n","* [Link](https://wandb.ai/neural-network-tab-bert/CreditTabBert) to the wandb **project**\n","\n","* MLM task training/evaluation plots can be seen at the following [link](\n","https://api.wandb.ai/links/neural-network-tab-bert/cwliqy1o)\n","\n","* CLASSIFICATION task training/evaluation plots can be seen at the following [link](\n","https://api.wandb.ai/links/neural-network-tab-bert/1lff34z1)"]},{"cell_type":"markdown","metadata":{"id":"PHlbS-JiIXsD"},"source":["---\n","# **REFERENCES**\n","\n","- Inkit Padhi, Yair Schiff, Igor Melnyk, Mattia Rigotti, Youssef Mroueh, Pierre Dognin, Jerret Ross, Ravi Nair, and Erik Altman. \"Tabular Transformers for Modeling Multivariate Time Series\". 2021. arXiv:2011.01843 [cs.LG].\n","\n"]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":["wc_PufOvIo2G","1TEL51zHSHej","6-uPFoOZfDBN","FHPOff5z7ZFX","W6J9_N8qBc7A"],"gpuType":"T4","provenance":[{"file_id":"1up72qIv8AUCnTsZ4G31kcf0RER8KMRE1","timestamp":1696925812051}]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}
